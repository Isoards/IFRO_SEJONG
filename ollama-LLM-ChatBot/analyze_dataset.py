#!/usr/bin/env python3
"""
ë°ì´í„°ì…‹ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì˜ë„ ë¶„ë¥˜ í•™ìŠµ ë°ì´í„°ì…‹ì„ ë¶„ì„í•˜ì—¬ í†µê³„ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""

import json
import os
from collections import Counter
from typing import Dict, List, Tuple

def analyze_dataset(file_path: str) -> Dict:
    """ë°ì´í„°ì…‹ ë¶„ì„"""
    print("=" * 60)
    print("ğŸ“Š ë°ì´í„°ì…‹ ë¶„ì„")
    print("=" * 60)
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            dataset = json.load(f)
    except Exception as e:
        print(f"âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}
    
    # ê¸°ë³¸ í†µê³„
    total_intents = len(dataset)
    total_examples = sum(len(examples) for examples in dataset.values())
    
    print(f"\nğŸ“ˆ ê¸°ë³¸ í†µê³„")
    print(f"ì˜ë„ ìœ í˜• ìˆ˜: {total_intents}")
    print(f"ì´ ì˜ˆì‹œ ìˆ˜: {total_examples}")
    print(f"í‰ê·  ì˜ˆì‹œ ìˆ˜: {total_examples / total_intents:.1f}")
    
    # ì˜ë„ë³„ í†µê³„
    print(f"\nğŸ“‹ ì˜ë„ë³„ í†µê³„")
    intent_stats = []
    for intent, examples in dataset.items():
        count = len(examples)
        percentage = (count / total_examples) * 100
        intent_stats.append((intent, count, percentage))
        
        print(f"{intent}: {count}ê°œ ({percentage:.1f}%)")
    
    # ì˜ë„ë³„ ì˜ˆì‹œ ìˆ˜ ë¶„í¬
    print(f"\nğŸ“Š ì˜ë„ë³„ ì˜ˆì‹œ ìˆ˜ ë¶„í¬")
    counts = [count for _, count, _ in intent_stats]
    print(f"ìµœì†Œ: {min(counts)}ê°œ")
    print(f"ìµœëŒ€: {max(counts)}ê°œ")
    print(f"ì¤‘ì•™ê°’: {sorted(counts)[len(counts)//2]}ê°œ")
    
    # ë¶ˆê· í˜• ë¶„ì„
    print(f"\nâš–ï¸ ë°ì´í„° ë¶ˆê· í˜• ë¶„ì„")
    max_count = max(counts)
    min_count = min(counts)
    imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')
    print(f"ìµœëŒ€/ìµœì†Œ ë¹„ìœ¨: {imbalance_ratio:.1f}")
    
    if imbalance_ratio > 3:
        print("âš ï¸ ë°ì´í„° ë¶ˆê· í˜•ì´ ì‹¬ê°í•©ë‹ˆë‹¤. ì¼ë¶€ ì˜ë„ì— ëŒ€í•œ ì¶”ê°€ ë°ì´í„°ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    elif imbalance_ratio > 2:
        print("âš ï¸ ë°ì´í„° ë¶ˆê· í˜•ì´ ìˆìŠµë‹ˆë‹¤. ì¼ë¶€ ì˜ë„ì— ëŒ€í•œ ì¶”ê°€ ë°ì´í„°ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”.")
    else:
        print("âœ… ë°ì´í„° ë¶„í¬ê°€ ë¹„êµì  ê· í˜•ì¡í˜€ ìˆìŠµë‹ˆë‹¤.")
    
    # ì§ˆë¬¸ ê¸¸ì´ ë¶„ì„
    print(f"\nğŸ“ ì§ˆë¬¸ ê¸¸ì´ ë¶„ì„")
    all_questions = []
    for examples in dataset.values():
        all_questions.extend(examples)
    
    question_lengths = [len(q) for q in all_questions]
    avg_length = sum(question_lengths) / len(question_lengths)
    min_length = min(question_lengths)
    max_length = max(question_lengths)
    
    print(f"í‰ê·  ê¸¸ì´: {avg_length:.1f}ì")
    print(f"ìµœì†Œ ê¸¸ì´: {min_length}ì")
    print(f"ìµœëŒ€ ê¸¸ì´: {max_length}ì")
    
    # ê¸¸ì´ë³„ ë¶„í¬
    length_ranges = [
        (0, 10, "ë§¤ìš° ì§§ìŒ"),
        (11, 20, "ì§§ìŒ"),
        (21, 40, "ë³´í†µ"),
        (41, 60, "ê¸¸ìŒ"),
        (61, float('inf'), "ë§¤ìš° ê¸¸ìŒ")
    ]
    
    print(f"\nğŸ“ ê¸¸ì´ë³„ ë¶„í¬")
    for min_len, max_len, label in length_ranges:
        if max_len == float('inf'):
            count = sum(1 for l in question_lengths if l >= min_len)
        else:
            count = sum(1 for l in question_lengths if min_len <= l <= max_len)
        percentage = (count / len(question_lengths)) * 100
        print(f"{label} ({min_len}-{max_len if max_len != float('inf') else 'âˆ'}ì): {count}ê°œ ({percentage:.1f}%)")
    
    # í‚¤ì›Œë“œ ë¶„ì„
    print(f"\nğŸ” í‚¤ì›Œë“œ ë¶„ì„")
    common_keywords = analyze_keywords(all_questions)
    print("ê°€ì¥ ìì£¼ ì‚¬ìš©ë˜ëŠ” í‚¤ì›Œë“œ (ìƒìœ„ 10ê°œ):")
    for keyword, count in common_keywords[:10]:
        print(f"  {keyword}: {count}íšŒ")
    
    # ì˜ë„ë³„ í‚¤ì›Œë“œ ë¶„ì„
    print(f"\nğŸ¯ ì˜ë„ë³„ í‚¤ì›Œë“œ ë¶„ì„")
    for intent, examples in dataset.items():
        intent_keywords = analyze_keywords(examples)
        print(f"\n{intent} (ìƒìœ„ 5ê°œ):")
        for keyword, count in intent_keywords[:5]:
            print(f"  {keyword}: {count}íšŒ")
    
    return {
        "total_intents": total_intents,
        "total_examples": total_examples,
        "intent_stats": intent_stats,
        "imbalance_ratio": imbalance_ratio,
        "avg_question_length": avg_length,
        "common_keywords": common_keywords[:10]
    }

def analyze_keywords(questions: List[str]) -> List[Tuple[str, int]]:
    """í‚¤ì›Œë“œ ë¶„ì„"""
    # í•œêµ­ì–´ ë¶ˆìš©ì–´
    stopwords = {
        'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ',
        'ì™€', 'ê³¼', 'ë„', 'ë§Œ', 'ê¹Œì§€', 'ë¶€í„°', 'ê»˜ì„œ', 'í•œí…Œ', 'ì—ê²Œ',
        'ê·¸', 'ì €', 'ì´', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ì´ê²ƒ', 'ê²ƒ', 'ìˆ˜', 'ë•Œ',
        'ìˆë‹¤', 'ì—†ë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ì•Šë‹¤', 'ëª»í•˜ë‹¤',
        'ì–´ë–¤', 'ì–´ë–»ê²Œ', 'ë¬´ì—‡', 'ë­', 'ì–´ë””', 'ì–¸ì œ', 'ëˆ„ê°€', 'ì™œ', 'ì–´ë–¤',
        'ì´ëŸ°', 'ì €ëŸ°', 'ê·¸ëŸ°', 'ì´ëŸ¬í•œ', 'ì €ëŸ¬í•œ', 'ê·¸ëŸ¬í•œ'
    }
    
    # í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = []
    for question in questions:
        # ê°„ë‹¨í•œ í† í°í™” (ê³µë°± ê¸°ì¤€)
        tokens = question.split()
        for token in tokens:
            # íŠ¹ìˆ˜ë¬¸ì ì œê±°
            clean_token = ''.join(c for c in token if c.isalnum() or c in 'ê°€-í£')
            if clean_token and len(clean_token) > 1 and clean_token not in stopwords:
                keywords.append(clean_token)
    
    # ë¹ˆë„ ê³„ì‚°
    keyword_counts = Counter(keywords)
    return keyword_counts.most_common()

def suggest_improvements(analysis_result: Dict):
    """ê°œì„  ì œì•ˆ"""
    print(f"\nğŸ’¡ ê°œì„  ì œì•ˆ")
    
    # ë°ì´í„° ë¶ˆê· í˜• ê°œì„  ì œì•ˆ
    if analysis_result.get('imbalance_ratio', 0) > 2:
        print("1. ë°ì´í„° ë¶ˆê· í˜• ê°œì„ :")
        print("   - ì˜ˆì‹œê°€ ì ì€ ì˜ë„ì— ëŒ€í•œ ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘")
        print("   - ë°ì´í„° ì¦ê°• ê¸°ë²• í™œìš©")
        print("   - ê°€ì¤‘ì¹˜ ê¸°ë°˜ í•™ìŠµ ê³ ë ¤")
    
    # ì§ˆë¬¸ ê¸¸ì´ ë‹¤ì–‘ì„± ê°œì„  ì œì•ˆ
    avg_length = analysis_result.get('avg_question_length', 0)
    if avg_length < 15:
        print("2. ì§ˆë¬¸ ê¸¸ì´ ë‹¤ì–‘ì„± ê°œì„ :")
        print("   - ë” ìƒì„¸í•œ ì§ˆë¬¸ ì˜ˆì‹œ ì¶”ê°€")
        print("   - ë³µí•© ì§ˆë¬¸ ì˜ˆì‹œ í¬í•¨")
    
    # í‚¤ì›Œë“œ ë‹¤ì–‘ì„± ê°œì„  ì œì•ˆ
    common_keywords = analysis_result.get('common_keywords', [])
    if len(common_keywords) < 20:
        print("3. í‚¤ì›Œë“œ ë‹¤ì–‘ì„± ê°œì„ :")
        print("   - ë™ì˜ì–´, ìœ ì‚¬ì–´ í™œìš©")
        print("   - ë‹¤ì–‘í•œ í‘œí˜„ ë°©ì‹ í¬í•¨")
    
    print("4. ì¼ë°˜ì ì¸ ê°œì„  ì‚¬í•­:")
    print("   - ì‹¤ì œ ì‚¬ìš©ì ì§ˆë¬¸ íŒ¨í„´ ë°˜ì˜")
    print("   - ë„ë©”ì¸ íŠ¹í™” ìš©ì–´ ì¶”ê°€")
    print("   - ì˜¤íƒ€, ì¶•ì•½í˜• ë“± ì‹¤ì œ ì‚¬ìš© íŒ¨í„´ í¬í•¨")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    dataset_path = "data/intent_training_dataset.json"
    
    if not os.path.exists(dataset_path):
        print(f"âŒ ë°ì´í„°ì…‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dataset_path}")
        return
    
    # ë°ì´í„°ì…‹ ë¶„ì„
    analysis_result = analyze_dataset(dataset_path)
    
    if analysis_result:
        # ê°œì„  ì œì•ˆ
        suggest_improvements(analysis_result)
        
        print(f"\n" + "=" * 60)
        print("âœ… ë°ì´í„°ì…‹ ë¶„ì„ ì™„ë£Œ!")
        print("=" * 60)
    else:
        print("âŒ ë°ì´í„°ì…‹ ë¶„ì„ ì‹¤íŒ¨")

if __name__ == "__main__":
    main()
