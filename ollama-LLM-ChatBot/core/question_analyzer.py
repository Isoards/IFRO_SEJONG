"""
ì§ˆë¬¸ ë¶„ì„ ëª¨ë“ˆ (ìµœì í™” ë²„ì „)

ë¹ ë¥¸ ì§ˆë¬¸ ë¶„ì„ì„ ìœ„í•œ ê°„ì†Œí™”ëœ ë¶„ì„ê¸°
"""

import re
from typing import List, Dict, Optional, Any
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
import numpy as np
from sentence_transformers import SentenceTransformer
import logging

logger = logging.getLogger(__name__)

class QuestionType(Enum):
    """ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ (ë‹¨ìˆœí™”)"""
    GREETING = "greeting"            # ì¸ì‚¬ë§
    FACTUAL = "factual"              # ì‚¬ì‹¤ ì§ˆë¬¸
    CONCEPTUAL = "conceptual"        # ê°œë… ì§ˆë¬¸
    DATABASE_QUERY = "database_query"  # ë°ì´í„°ë² ì´ìŠ¤ ì§ˆì˜
    QUANTITATIVE = "quantitative"    # ì •ëŸ‰ì  ì§ˆë¬¸
    UNKNOWN = "unknown"              # ì•Œ ìˆ˜ ì—†ìŒ

@dataclass
class ConversationItem:
    """ëŒ€í™” í•­ëª© ë°ì´í„° í´ëž˜ìŠ¤"""
    question: str
    answer: str
    timestamp: datetime
    question_type: QuestionType
    relevant_chunks: List[str]
    confidence_score: float = 0.0
    metadata: Optional[Dict] = None

@dataclass 
class AnalyzedQuestion:
    """ë¶„ì„ëœ ì§ˆë¬¸ ë°ì´í„° í´ëž˜ìŠ¤ (ë‹¨ìˆœí™”)"""
    original_question: str
    processed_question: str
    question_type: QuestionType
    keywords: List[str]
    entities: List[str]
    intent: str
    context_keywords: List[str]
    requires_sql: bool = False
    sql_intent: Optional[str] = None
    embedding: Optional[np.ndarray] = None
    metadata: Optional[Dict] = None
    enhanced_question: Optional[str] = None

class QuestionAnalyzer:
    """ì§ˆë¬¸ ë¶„ì„ê¸° (ìµœì í™”)"""
    
    def __init__(self, embedding_model: str = "jhgan/ko-sroberta-multitask"):
        """QuestionAnalyzer ì´ˆê¸°í™”"""
        # ìž„ë² ë”© ëª¨ë¸ ë¡œë“œ
        try:
            self.embedding_model = SentenceTransformer(embedding_model)
            logger.info(f"ì§ˆë¬¸ ë¶„ì„ìš© ìž„ë² ë”© ëª¨ë¸ ë¡œë“œ: {embedding_model}")
        except Exception as e:
            logger.error(f"ìž„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.embedding_model = None
        
        # ëŒ€í™” ížˆìŠ¤í† ë¦¬ (ë‹¨ìˆœí™”)
        self.conversation_history: List[ConversationItem] = []
        
        # ì§ˆë¬¸ ìœ í˜• íŒ¨í„´ (ë‹¨ìˆœí™”)
        self.question_patterns = {
            QuestionType.GREETING: [
                r'ì•ˆë…•', r'ë°˜ê°‘', r'í•˜ì´', r'ì²˜ìŒ', r'ë„ì›€'
            ],
            QuestionType.FACTUAL: [
                r'ë¬´ì—‡', r'ì–¸ì œ', r'ì–´ë””ì„œ', r'ëˆ„ê°€', r'ì–´ë–¤'
            ],
            QuestionType.CONCEPTUAL: [
                r'ì–´ë–»ê²Œ', r'ì™œ', r'ì›ë¦¬', r'ê°œë…', r'ì •ì˜'
            ],
            QuestionType.DATABASE_QUERY: [
                r'ëª‡', r'ê°œìˆ˜', r'ê±´ìˆ˜', r'ì´', r'í‰ê· ', r'ìµœëŒ€', r'ìµœì†Œ',
                r'êµí†µëŸ‰', r'í†µí–‰ëŸ‰', r'ì‚¬ê³ ', r'êµ¬ë³„', r'ì§€ì—­ë³„', r'í†µê³„'
            ],
            QuestionType.QUANTITATIVE: [
                r'ì–¼ë§ˆë‚˜', r'ë¹„ìœ¨', r'ìˆœìœ„', r'ë¶„ì„', r'ë°ì´í„°'
            ]
        }
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ íŒ¨í„´
        self.keyword_patterns = [
            r'\b\w+êµ¬\b',  # ì§€ì—­ëª…
            r'\b\w+êµì°¨ë¡œ\b',  # êµì°¨ë¡œëª…
            r'\bêµí†µëŸ‰\b', r'\bí†µí–‰ëŸ‰\b',  # êµí†µ ê´€ë ¨
            r'\bì‚¬ê³ \b', r'\bì ‘ì´‰ì‚¬ê³ \b',  # ì‚¬ê³  ê´€ë ¨
            r'\bì‹ í˜¸\b', r'\bì‹ í˜¸ë“±\b',  # ì‹ í˜¸ ê´€ë ¨
            r'\bIFRO\b', r'\bì‹œìŠ¤í…œ\b'  # ì‹œìŠ¤í…œ ê´€ë ¨
        ]
        
        logger.info("ì§ˆë¬¸ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def analyze_question(self, question: str, use_conversation_context: bool = True) -> AnalyzedQuestion:
        """ì§ˆë¬¸ ë¶„ì„ (ìµœì í™”)"""
        import time
        total_start_time = time.time()
        
        # 1. ê¸°ë³¸ ì „ì²˜ë¦¬
        preprocess_start = time.time()
        processed_question = self._preprocess_question(question)
        preprocess_time = time.time() - preprocess_start
        
        # 2. ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜
        classify_start = time.time()
        question_type = self._classify_question_type(processed_question)
        classify_time = time.time() - classify_start
        
        # 3. í‚¤ì›Œë“œ ì¶”ì¶œ
        keyword_start = time.time()
        keywords = self._extract_keywords(processed_question)
        keyword_time = time.time() - keyword_start
        
        # 4. ê°œì²´ëª… ì¶”ì¶œ
        entity_start = time.time()
        entities = self._extract_entities(processed_question)
        entity_time = time.time() - entity_start
        
        # 5. ì˜ë„ ë¶„ì„
        intent_start = time.time()
        intent = self._analyze_intent(processed_question, question_type)
        intent_time = time.time() - intent_start
        
        # 6. ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ (ë‹¨ìˆœí™”)
        context_start = time.time()
        context_keywords = []
        if use_conversation_context and self.conversation_history:
            context_keywords = self._extract_context_keywords()
        context_time = time.time() - context_start
        
        # 7. SQL ìš”êµ¬ì‚¬í•­ í™•ì¸
        sql_start = time.time()
        requires_sql, sql_intent = self._check_sql_requirement(question_type, keywords)
        sql_time = time.time() - sql_start
        
        # 8. ìž„ë² ë”© ìƒì„± (ê°€ìž¥ ì˜¤ëž˜ ê±¸ë¦´ ìˆ˜ ìžˆëŠ” ë¶€ë¶„)
        embedding_start = time.time()
        embedding = None
        if self.embedding_model:
            try:
                embedding = self.embedding_model.encode([processed_question])[0]
            except Exception as e:
                logger.warning(f"ìž„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        embedding_time = time.time() - embedding_start
        
        # 9. í–¥ìƒëœ ì§ˆë¬¸ ìƒì„± (ë‹¨ìˆœí™”)
        enhance_start = time.time()
        enhanced_question = self._enhance_question(processed_question, context_keywords)
        enhance_time = time.time() - enhance_start
        
        # 10. ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata_start = time.time()
        total_time = time.time() - total_start_time
        metadata = {
            "processing_time": total_time,
            "question_length": len(question),
            "keywords_count": len(keywords),
            "entities_count": len(entities),
            "timing_breakdown": {
                "preprocess": preprocess_time,
                "classify": classify_time,
                "keyword_extract": keyword_time,
                "entity_extract": entity_time,
                "intent_analysis": intent_time,
                "context_keywords": context_time,
                "sql_check": sql_time,
                "embedding": embedding_time,
                "enhance": enhance_time
            }
        }
        metadata_time = time.time() - metadata_start
        
        analyzed_question = AnalyzedQuestion(
            original_question=question,
            processed_question=processed_question,
            question_type=question_type,
            keywords=keywords,
            entities=entities,
            intent=intent,
            context_keywords=context_keywords,
            requires_sql=requires_sql,
            sql_intent=sql_intent,
            embedding=embedding,
            enhanced_question=enhanced_question,
            metadata=metadata
        )
        
        print(f"  ðŸ” ë¶„ì„ ì„¸ë¶€: ì „ì²˜ë¦¬({preprocess_time:.3f}s) | ë¶„ë¥˜({classify_time:.3f}s) | í‚¤ì›Œë“œ({keyword_time:.3f}s) | ìž„ë² ë”©({embedding_time:.3f}s) | ê¸°íƒ€({(entity_time+intent_time+context_time+sql_time+enhance_time+metadata_time):.3f}s)")
        
        logger.info(f"ì§ˆë¬¸ ë¶„ì„ ì™„ë£Œ: {question_type.value}, í‚¤ì›Œë“œ: {len(keywords)}ê°œ")
        return analyzed_question
    
    def _preprocess_question(self, question: str) -> str:
        """ì§ˆë¬¸ ì „ì²˜ë¦¬"""
        # ê¸°ë³¸ ì •ê·œí™”
        processed = question.strip()
        processed = re.sub(r'\s+', ' ', processed)  # ì—°ì† ê³µë°± ì œê±°
        processed = re.sub(r'[^\w\sê°€-íž£]', '', processed)  # íŠ¹ìˆ˜ë¬¸ìž ì œê±° (í•œê¸€ ì œì™¸)
        return processed
    
    def _classify_question_type(self, question: str) -> QuestionType:
        """ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜"""
        question_lower = question.lower()
        
        # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ìœ í˜• ê²°ì •
        for question_type, patterns in self.question_patterns.items():
            for pattern in patterns:
                if re.search(pattern, question_lower):
                    return question_type
        
        return QuestionType.UNKNOWN
    
    def _extract_keywords(self, question: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        
        # íŒ¨í„´ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        for pattern in self.keyword_patterns:
            matches = re.findall(pattern, question, re.IGNORECASE)
            keywords.extend(matches)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        keywords = list(set(keywords))
        keywords.sort()
        
        return keywords
    
    def _extract_entities(self, question: str) -> List[str]:
        """ê°œì²´ëª… ì¶”ì¶œ (ë‹¨ìˆœí™”)"""
        entities = []
        
        # ì§€ì—­ëª… ì¶”ì¶œ
        location_pattern = r'\b\w+êµ¬\b'
        locations = re.findall(location_pattern, question)
        entities.extend(locations)
        
        # êµì°¨ë¡œëª… ì¶”ì¶œ
        intersection_pattern = r'\b\w+êµì°¨ë¡œ\b'
        intersections = re.findall(intersection_pattern, question)
        entities.extend(intersections)
        
        return list(set(entities))
    
    def _analyze_intent(self, question: str, question_type: QuestionType) -> str:
        """ì˜ë„ ë¶„ì„ (ë‹¨ìˆœí™”)"""
        if question_type == QuestionType.GREETING:
            return "ì¸ì‚¬"
        elif question_type == QuestionType.DATABASE_QUERY:
            return "ë°ì´í„°_ì¡°íšŒ"
        elif question_type == QuestionType.CONCEPTUAL:
            return "ê°œë…_ì„¤ëª…"
        elif question_type == QuestionType.FACTUAL:
            return "ì‚¬ì‹¤_ì¡°íšŒ"
        else:
            return "ì¼ë°˜_ì§ˆë¬¸"
    
    def _extract_context_keywords(self) -> List[str]:
        """ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ ì¶”ì¶œ (ë‹¨ìˆœí™”)"""
        if not self.conversation_history:
            return []
        
        # ìµœê·¼ 3ê°œ ëŒ€í™”ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        recent_keywords = []
        for item in self.conversation_history[-3:]:
            recent_keywords.extend(item.question.split()[:5])  # ìƒìœ„ 5ê°œ ë‹¨ì–´ë§Œ
        
        return list(set(recent_keywords))
    
    def _check_sql_requirement(self, question_type: QuestionType, keywords: List[str]) -> tuple[bool, Optional[str]]:
        """SQL ìš”êµ¬ì‚¬í•­ í™•ì¸"""
        if question_type == QuestionType.DATABASE_QUERY:
            return True, "SELECT"
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ í™•ì¸
        sql_keywords = ['êµí†µëŸ‰', 'í†µí–‰ëŸ‰', 'ì‚¬ê³ ', 'êµ¬ë³„', 'í†µê³„', 'ê°œìˆ˜', 'ê±´ìˆ˜']
        if any(keyword in keywords for keyword in sql_keywords):
            return True, "SELECT"
        
        return False, None
    
    def _enhance_question(self, question: str, context_keywords: List[str]) -> str:
        """ì§ˆë¬¸ í–¥ìƒ (ë‹¨ìˆœí™”)"""
        if not context_keywords:
            return question
        
        # ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œê°€ ì§ˆë¬¸ì— ì—†ìœ¼ë©´ ì¶”ê°€
        enhanced = question
        for keyword in context_keywords[:2]:  # ìµœëŒ€ 2ê°œë§Œ ì¶”ê°€
            if keyword not in question:
                enhanced += f" {keyword}"
        
        return enhanced
    

    
    def get_conversation_summary(self) -> Dict:
        """ëŒ€í™” ìš”ì•½ ë°˜í™˜"""
        return {
            "total_conversations": len(self.conversation_history),
            "recent_questions": [item.question for item in self.conversation_history[-3:]],
            "question_types": [item.question_type.value for item in self.conversation_history[-5:]]
        }
    
    def clear_conversation_history(self):
        """ëŒ€í™” ížˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.conversation_history.clear()
        logger.info("ëŒ€í™” ížˆìŠ¤í† ë¦¬ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def get_conversation_context(self, max_items: int = 3) -> List[Dict]:
        """ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜"""
        if not self.conversation_history:
            return []
        
        # ìµœê·¼ ëŒ€í™” í•­ëª©ë“¤ì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
        context = []
        for item in self.conversation_history[-max_items:]:
            context.append({
                "question": item.question,
                "answer": item.answer,
                "timestamp": item.timestamp.isoformat(),
                "question_type": item.question_type.value,
                "confidence_score": item.confidence_score
            })
        
        return context
    
    def add_conversation_item(self, question: str, answer: str, used_chunks: List[str], confidence_score: float):
        """ëŒ€í™” í•­ëª© ì¶”ê°€ (ë‹¨ìˆœí™”ëœ ë²„ì „)"""
        item = ConversationItem(
            question=question,
            answer=answer,
            timestamp=datetime.now(),
            question_type=QuestionType.UNKNOWN,  # ê¸°ë³¸ê°’
            relevant_chunks=used_chunks,
            confidence_score=confidence_score
        )
        self.conversation_history.append(item)
        
        # ížˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ (ìµœê·¼ 10ê°œë§Œ ìœ ì§€)
        if len(self.conversation_history) > 10:
            self.conversation_history = self.conversation_history[-10:]
