"""
ë²¡í„° ì €ì¥ì†Œ ë° ê²€ìƒ‰ ê¸°ëŠ¥ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ PDFì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì˜ ì„ë² ë”©ì„ ì €ì¥í•˜ê³ ,
ì§ˆë¬¸ì— ëŒ€í•´ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import os
import json
import pickle
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import asdict
import numpy as np
from abc import ABC, abstractmethod

import faiss
import chromadb
from chromadb.config import Settings
from sklearn.metrics.pairwise import cosine_similarity

from .pdf_processor import TextChunk

import logging
logger = logging.getLogger(__name__)

class VectorStoreInterface(ABC):
    """ë²¡í„° ì €ì¥ì†Œ ì¸í„°í˜ì´ìŠ¤"""
    
    @abstractmethod
    def add_chunks(self, chunks: List[TextChunk]) -> None:
        """í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ ì €ì¥ì†Œì— ì¶”ê°€"""
        pass
    
    @abstractmethod
    def search(self, query_embedding: np.ndarray, top_k: int = 5) -> List[Tuple[TextChunk, float]]:
        """ì¿¼ë¦¬ ì„ë² ë”©ê³¼ ìœ ì‚¬í•œ ì²­í¬ë“¤ì„ ê²€ìƒ‰"""
        pass
    
    @abstractmethod
    def save(self, path: str) -> None:
        """ì €ì¥ì†Œë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        pass
    
    @abstractmethod
    def load(self, path: str) -> None:
        """íŒŒì¼ì—ì„œ ì €ì¥ì†Œ ë¡œë“œ"""
        pass

class FAISSVectorStore(VectorStoreInterface):
    """
    FAISSë¥¼ ì‚¬ìš©í•œ ë²¡í„° ì €ì¥ì†Œ
    
    ì¥ì :
    - ë§¤ìš° ë¹ ë¥¸ ê²€ìƒ‰ ì†ë„
    - ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
    - ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ê°€ëŠ¥
    
    ë‹¨ì :
    - ë©”íƒ€ë°ì´í„° ê´€ë¦¬ê°€ ë³„ë„ë¡œ í•„ìš”
    - ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ê°€ ìƒëŒ€ì ìœ¼ë¡œ ë³µì¡
    """
    
    def __init__(self, embedding_dimension: int = 768):
        """
        FAISS ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
        
        Args:
            embedding_dimension: ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì›
        """
        self.embedding_dimension = embedding_dimension
        self.index = faiss.IndexFlatIP(embedding_dimension)  # Inner Product (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
        self.chunks: List[TextChunk] = []
        self.chunk_metadata: List[Dict] = []
        
        # ì •ê·œí™”ë¥¼ ìœ„í•œ ì„¤ì • (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ìš©)
        self.normalize_embeddings = True
        
        logger.info(f"FAISS ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì™„ë£Œ (ì°¨ì›: {embedding_dimension})")
    
    def add_chunks(self, chunks: List[TextChunk]) -> None:
        """
        í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
        
        Args:
            chunks: ì„ë² ë”©ì´ í¬í•¨ëœ TextChunk ë¦¬ìŠ¤íŠ¸
        """
        if not chunks:
            return
        
        # ì„ë² ë”© ë²¡í„° ì¶”ì¶œ ë° ì •ê·œí™”
        embeddings = []
        for chunk in chunks:
            if chunk.embedding is None:
                raise ValueError(f"ì²­í¬ {chunk.chunk_id}ì— ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            embedding = chunk.embedding.copy()
            if self.normalize_embeddings:
                # L2 ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ìš©)
                norm = np.linalg.norm(embedding)
                if norm > 0:
                    embedding = embedding / norm
            
            embeddings.append(embedding)
        
        # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
        embeddings_matrix = np.array(embeddings).astype('float32')
        self.index.add(embeddings_matrix)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        self.chunks.extend(chunks)
        for chunk in chunks:
            metadata = asdict(chunk)
            metadata.pop('embedding', None)  # ì„ë² ë”©ì€ ë³„ë„ ì €ì¥
            self.chunk_metadata.append(metadata)
        
        logger.info(f"{len(chunks)}ê°œ ì²­í¬ë¥¼ FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€ (ì´ {len(self.chunks)}ê°œ)")
    
    def search(self, query_embedding: np.ndarray, top_k: int = 5, 
               use_hybrid_search: bool = True) -> List[Tuple[TextChunk, float]]:
        """
        ì¿¼ë¦¬ ì„ë² ë”©ê³¼ ìœ ì‚¬í•œ ì²­í¬ë“¤ì„ ê²€ìƒ‰ (ê°œì„ ëœ ë²„ì „)
        
        Args:
            query_embedding: ì¿¼ë¦¬ ì„ë² ë”© ë²¡í„°
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            use_hybrid_search: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            (TextChunk, ìœ ì‚¬ë„ ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        import time
        search_start = time.time()
        
        if len(self.chunks) == 0:
            return []
        
        # 1. ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
        vector_start = time.time()
        if self.normalize_embeddings:
            query_embedding = query_embedding / np.linalg.norm(query_embedding)
        
        # FAISS ê²€ìƒ‰
        scores, indices = self.index.search(
            query_embedding.reshape(1, -1), 
            min(top_k * 2, len(self.chunks))  # ë” ë§ì€ í›„ë³´ ê²€ìƒ‰
        )
        vector_time = time.time() - vector_start
        
        # 2. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í‚¤ì›Œë“œ ë§¤ì¹­ + ë²¡í„° ìœ ì‚¬ë„)
        hybrid_start = time.time()
        if use_hybrid_search:
            results = self._hybrid_search(query_embedding, scores[0], indices[0], top_k)
        else:
            results = [(self.chunks[i], float(s)) for s, i in zip(scores[0], indices[0])]
        hybrid_time = time.time() - hybrid_start
        
        # 3. ê²°ê³¼ í•„í„°ë§ ë° ì •ë ¬
        filter_start = time.time()
        filtered_results = self._filter_and_rank_results(results, top_k)
        filter_time = time.time() - filter_start
        
        total_time = time.time() - search_start
        print(f"    ğŸ“Š FAISS ê²€ìƒ‰ ì„¸ë¶€: ë²¡í„°({vector_time:.3f}s) | í•˜ì´ë¸Œë¦¬ë“œ({hybrid_time:.3f}s) | í•„í„°({filter_time:.3f}s) | ì´({total_time:.3f}s)")
        
        return filtered_results[:top_k]
    
    def _hybrid_search(self, query_embedding: np.ndarray, 
                      vector_scores: np.ndarray, 
                      vector_indices: np.ndarray,
                      top_k: int) -> List[Tuple[TextChunk, float]]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° ìœ ì‚¬ë„ + í‚¤ì›Œë“œ ë§¤ì¹­)
        
        Args:
            query_embedding: ì¿¼ë¦¬ ì„ë² ë”©
            vector_scores: ë²¡í„° ìœ ì‚¬ë„ ì ìˆ˜ë“¤
            vector_indices: ë²¡í„° ì¸ë±ìŠ¤ë“¤
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë¡œ ì •ë ¬ëœ ê²°ê³¼ë“¤
        """
        hybrid_results = []
        
        for score, idx in zip(vector_scores, vector_indices):
            chunk = self.chunks[idx]
            
            # ë²¡í„° ìœ ì‚¬ë„ ì ìˆ˜ (0.6 ê°€ì¤‘ì¹˜)
            vector_score = float(score) * 0.6
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ (0.4 ê°€ì¤‘ì¹˜)
            keyword_score = self._calculate_keyword_score(chunk) * 0.4
            
            # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜
            hybrid_score = vector_score + keyword_score
            
            hybrid_results.append((chunk, hybrid_score))
        
        # ì ìˆ˜ë¡œ ì •ë ¬
        hybrid_results.sort(key=lambda x: x[1], reverse=True)
        
        return hybrid_results
    
    def _calculate_keyword_score(self, chunk: TextChunk) -> float:
        """
        ì²­í¬ì˜ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
        
        Args:
            chunk: í‰ê°€í•  í…ìŠ¤íŠ¸ ì²­í¬
            
        Returns:
            í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ (0.0 ~ 1.0)
        """
        # ì¤‘ìš” í‚¤ì›Œë“œ íŒ¨í„´ (ë„ë©”ì¸ë³„ë¡œ í™•ì¥ ê°€ëŠ¥)
        important_keywords = [
            'ì‹œìŠ¤í…œ', 'í”„ë¡œê·¸ë¨', 'ì†Œí”„íŠ¸ì›¨ì–´', 'ì• í”Œë¦¬ì¼€ì´ì…˜',
            'ë°ì´í„°', 'ì •ë³´', 'ìë£Œ', 'íŒŒì¼',
            'ì‚¬ìš©ì', 'ê´€ë¦¬ì', 'ê³ ê°', 'ì´ìš©ì',
            'ë³´ì•ˆ', 'ì¸ì¦', 'ê¶Œí•œ', 'ì ‘ê·¼',
            'ì„±ëŠ¥', 'ì†ë„', 'íš¨ìœ¨', 'í’ˆì§ˆ',
            'ì˜¤ë¥˜', 'ë¬¸ì œ', 'ì¥ì• ', 'í•´ê²°',
            'ì„¤ì •', 'êµ¬ì„±', 'í™˜ê²½', 'ì˜µì…˜',
            'ë°±ì—…', 'ë³µêµ¬', 'ì €ì¥', 'ë³´ê´€',
            'ë„¤íŠ¸ì›Œí¬', 'í†µì‹ ', 'ì—°ê²°', 'ì „ì†¡',
            'ë°ì´í„°ë² ì´ìŠ¤', 'DB', 'í…Œì´ë¸”', 'ì¿¼ë¦¬',
            # êµí†µ ì‹œìŠ¤í…œ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ê°€
            'ë¶„ì„', 'ë·°', 'í™œì„±í™”', 'êµì°¨ë¡œ', 'êµí†µ',
            'ë„¤ë¹„ê²Œì´ì…˜', 'ì‚¬ì´ë“œë°”', 'ëª©ë¡', 'ê²€ìƒ‰',
            'ì„ íƒ', 'ë§ˆì»¤', 'ì§€ë„', 'íŒ¨ë„',
            'ë²„íŠ¼', 'í´ë¦­', 'ìœ„ì¹˜', 'ë°©ë²•',
            'ì¢Œì¸¡', 'ìš°ì¸¡', 'ìƒë‹¨', 'í•˜ë‹¨'
        ]
        
        content_lower = chunk.content.lower()
        score = 0.0
        
        # ì¤‘ìš” í‚¤ì›Œë“œ ë§¤ì¹­
        for keyword in important_keywords:
            if keyword in content_lower:
                score += 0.1
        
        # í‚¤ì›Œë“œ ë°€ë„ ê³„ì‚°
        total_words = len(content_lower.split())
        if total_words > 0:
            keyword_density = score / total_words
            score += keyword_density * 0.5
        
        return min(score, 1.0)
    
    def _filter_and_rank_results(self, results: List[Tuple[TextChunk, float]], 
                                top_k: int) -> List[Tuple[TextChunk, float]]:
        """
        ê²€ìƒ‰ ê²°ê³¼ í•„í„°ë§ ë° ì¬ì •ë ¬
        
        Args:
            results: ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            í•„í„°ë§ ë° ì¬ì •ë ¬ëœ ê²°ê³¼
        """
        filtered_results = []
        
        for chunk, score in results:
            # ìµœì†Œ ì ìˆ˜ ì„ê³„ê°’
            if score < 0.1:
                continue
            
            # ì¤‘ë³µ ë‚´ìš© í•„í„°ë§
            is_duplicate = False
            for existing_chunk, _ in filtered_results:
                if self._is_similar_content(chunk.content, existing_chunk.content):
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                filtered_results.append((chunk, score))
            
            if len(filtered_results) >= top_k:
                break
        
        return filtered_results
    
    def _is_similar_content(self, content1: str, content2: str, 
                           similarity_threshold: float = 0.8) -> bool:
        """
        ë‘ í…ìŠ¤íŠ¸ ë‚´ìš©ì˜ ìœ ì‚¬ë„ íŒë‹¨
        
        Args:
            content1: ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸
            content2: ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            
        Returns:
            ìœ ì‚¬í•œ ë‚´ìš©ì¸ì§€ ì—¬ë¶€
        """
        # ê°„ë‹¨í•œ Jaccard ìœ ì‚¬ë„ ê³„ì‚°
        words1 = set(content1.lower().split())
        words2 = set(content2.lower().split())
        
        if not words1 or not words2:
            return False
        
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        similarity = intersection / union if union > 0 else 0.0
        
        return similarity >= similarity_threshold
    
    def save(self, path: str) -> None:
        """
        FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            path: ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        os.makedirs(path, exist_ok=True)
        
        # FAISS ì¸ë±ìŠ¤ ì €ì¥
        faiss.write_index(self.index, os.path.join(path, "faiss_index.bin"))
        
        # ì²­í¬ ë©”íƒ€ë°ì´í„° ì €ì¥
        with open(os.path.join(path, "chunks.pkl"), "wb") as f:
            pickle.dump(self.chunks, f)
        
        # ì¶”ê°€ ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            "embedding_dimension": self.embedding_dimension,
            "total_chunks": len(self.chunks),
            "normalize_embeddings": self.normalize_embeddings
        }
        with open(os.path.join(path, "metadata.json"), "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        logger.info(f"FAISS ë²¡í„° ì €ì¥ì†Œë¥¼ {path}ì— ì €ì¥ ì™„ë£Œ")
    
    def load(self, path: str) -> None:
        """
        íŒŒì¼ì—ì„œ FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë“œ
        
        Args:
            path: ë¡œë“œí•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(os.path.join(path, "metadata.json"), "r", encoding="utf-8") as f:
            metadata = json.load(f)
        
        self.embedding_dimension = metadata["embedding_dimension"]
        self.normalize_embeddings = metadata.get("normalize_embeddings", True)
        
        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        self.index = faiss.read_index(os.path.join(path, "faiss_index.bin"))
        
        # ì²­í¬ ë°ì´í„° ë¡œë“œ
        with open(os.path.join(path, "chunks.pkl"), "rb") as f:
            self.chunks = pickle.load(f)
        
        logger.info(f"FAISS ë²¡í„° ì €ì¥ì†Œë¥¼ {path}ì—ì„œ ë¡œë“œ ì™„ë£Œ ({len(self.chunks)}ê°œ ì²­í¬)")

class ChromaDBVectorStore(VectorStoreInterface):
    """
    ChromaDBë¥¼ ì‚¬ìš©í•œ ë²¡í„° ì €ì¥ì†Œ
    
    ì¥ì :
    - ë©”íƒ€ë°ì´í„° ê´€ë¦¬ê°€ í¸ë¦¬
    - ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ì§€ì›
    - í•„í„°ë§ ê¸°ëŠ¥ ì§€ì›
    
    ë‹¨ì :
    - FAISSë³´ë‹¤ ê²€ìƒ‰ ì†ë„ê°€ ìƒëŒ€ì ìœ¼ë¡œ ëŠë¦¼
    - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ìƒëŒ€ì ìœ¼ë¡œ ë§ìŒ
    """
    
    def __init__(self, collection_name: str = "pdf_chunks", persist_directory: str = "./chroma_db"):
        """
        ChromaDB ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
        
        Args:
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
            persist_directory: ë°ì´í„° ì§€ì†ì„±ì„ ìœ„í•œ ë””ë ‰í† ë¦¬
        """
        self.collection_name = collection_name
        self.persist_directory = persist_directory
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
        try:
            self.collection = self.client.get_collection(collection_name)
            logger.info(f"ê¸°ì¡´ ChromaDB ì»¬ë ‰ì…˜ ë¡œë“œ: {collection_name}")
        except:
            self.collection = self.client.create_collection(
                name=collection_name,
                metadata={"description": "PDF í…ìŠ¤íŠ¸ ì²­í¬ ì €ì¥ì†Œ"}
            )
            logger.info(f"ìƒˆ ChromaDB ì»¬ë ‰ì…˜ ìƒì„±: {collection_name}")
    
    def add_chunks(self, chunks: List[TextChunk]) -> None:
        """
        í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ ChromaDBì— ì¶”ê°€
        
        Args:
            chunks: ì„ë² ë”©ì´ í¬í•¨ëœ TextChunk ë¦¬ìŠ¤íŠ¸
        """
        if not chunks:
            return
        
        # ChromaDB í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ë³€í™˜
        ids = []
        embeddings = []
        documents = []
        metadatas = []
        
        # ì¤‘ë³µ ID í•„í„°ë§
        existing_ids = set()
        try:
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ì˜ ëª¨ë“  ID ê°€ì ¸ì˜¤ê¸°
            existing_data = self.collection.get()
            if existing_data and existing_data['ids']:
                existing_ids = set(existing_data['ids'])
        except Exception as e:
            logger.warning(f"ê¸°ì¡´ ID í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        new_chunks = []
        for chunk in chunks:
            if chunk.embedding is None:
                raise ValueError(f"ì²­í¬ {chunk.chunk_id}ì— ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            # ì¤‘ë³µ IDì¸ì§€ í™•ì¸
            if chunk.chunk_id in existing_ids:
                logger.info(f"ì¤‘ë³µ ID ê±´ë„ˆë›°ê¸°: {chunk.chunk_id}")
                continue
            
            ids.append(chunk.chunk_id)
            embeddings.append(chunk.embedding.tolist())
            documents.append(chunk.content)
            
            metadata = {
                "page_number": chunk.page_number,
                "pdf_id": chunk.metadata.get("pdf_id") if chunk.metadata else "",
                "chunk_index": chunk.metadata.get("chunk_index") if chunk.metadata else 0
            }
            metadatas.append(metadata)
            new_chunks.append(chunk)
        
        if not new_chunks:
            logger.info("ì¶”ê°€í•  ìƒˆë¡œìš´ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ChromaDBì— ì¶”ê°€
        try:
            self.collection.add(
                ids=ids,
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas
            )
            logger.info(f"{len(new_chunks)}ê°œ ì²­í¬ë¥¼ ChromaDBì— ì¶”ê°€")
        except Exception as e:
            logger.error(f"ChromaDBì— ì²­í¬ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            raise
    
    def search(self, query_embedding: np.ndarray, top_k: int = 5, 
               filter_metadata: Optional[Dict] = None) -> List[Tuple[TextChunk, float]]:
        """
        ì¿¼ë¦¬ ì„ë² ë”©ê³¼ ìœ ì‚¬í•œ ì²­í¬ë“¤ì„ ê²€ìƒ‰
        
        Args:
            query_embedding: ì¿¼ë¦¬ì˜ ì„ë² ë”© ë²¡í„°
            top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ê°œìˆ˜
            filter_metadata: ë©”íƒ€ë°ì´í„° í•„í„° ì¡°ê±´
            
        Returns:
            (TextChunk, ìœ ì‚¬ë„_ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        # ChromaDB ê²€ìƒ‰
        results = self.collection.query(
            query_embeddings=[query_embedding.tolist()],
            n_results=top_k,
            where=filter_metadata
        )
        
        # TextChunk ê°ì²´ë¡œ ë³€í™˜
        chunks_with_scores = []
        
        if results['ids'] and len(results['ids'][0]) > 0:
            for i in range(len(results['ids'][0])):
                chunk_id = results['ids'][0][i]
                content = results['documents'][0][i]
                metadata = results['metadatas'][0][i]
                distance = results['distances'][0][i]
                
                # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (ChromaDBëŠ” ê±°ë¦¬ ê¸°ë°˜)
                similarity_score = 1.0 - distance
                
                chunk = TextChunk(
                    content=content,
                    page_number=metadata.get('page_number', 1),
                    chunk_id=chunk_id,
                    metadata=metadata
                )
                
                chunks_with_scores.append((chunk, similarity_score))
        
        return chunks_with_scores
    
    def save(self, path: str) -> None:
        """ChromaDBëŠ” ìë™ìœ¼ë¡œ ì§€ì†ì„± ê´€ë¦¬"""
        logger.info("ChromaDBëŠ” ìë™ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì§€ì†ì ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.")
    
    def load(self, path: str) -> None:
        """ChromaDBëŠ” ì´ˆê¸°í™” ì‹œ ìë™ìœ¼ë¡œ ë¡œë“œë¨"""
        logger.info("ChromaDBëŠ” ì´ˆê¸°í™” ì‹œ ìë™ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")

class HybridVectorStore:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ë²¡í„° ì €ì¥ì†Œ
    
    FAISSì™€ ChromaDBë¥¼ ë³‘í–‰ ì‚¬ìš©í•˜ì—¬ ë¹ ë¥¸ ê²€ìƒ‰ê³¼ ë©”íƒ€ë°ì´í„° ê´€ë¦¬ë¥¼ ëª¨ë‘ ì œê³µí•©ë‹ˆë‹¤.
    ë‹¤ì¤‘ ì„ë² ë”© ëª¨ë¸ê³¼ í‘œí˜„ë³„ ì¸ë±ì‹±ì„ ì§€ì›í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, 
                 faiss_store: Optional[FAISSVectorStore] = None,
                 chroma_store: Optional[ChromaDBVectorStore] = None,
                 embedding_models: Optional[List[str]] = None,
                 primary_model: str = "jhgan/ko-sroberta-multitask"):
        """
        HybridVectorStore ì´ˆê¸°í™”
        
        Args:
            faiss_store: FAISS ë²¡í„° ì €ì¥ì†Œ
            chroma_store: ChromaDB ë²¡í„° ì €ì¥ì†Œ
            embedding_models: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
            primary_model: ì£¼ ì„ë² ë”© ëª¨ë¸
        """
        self.faiss_store = faiss_store or FAISSVectorStore()
        self.chroma_store = chroma_store or ChromaDBVectorStore()
        
        # ë‹¤ì¤‘ ì„ë² ë”© ëª¨ë¸ ì„¤ì •
        self.embedding_models = embedding_models or [
            "jhgan/ko-sroberta-multitask",  # í•œêµ­ì–´ íŠ¹í™”
            "all-MiniLM-L6-v2",            # ì˜ì–´/ë‹¤êµ­ì–´
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"  # ë‹¤êµ­ì–´
        ]
        self.primary_model = primary_model
        
        # í‘œí˜„ë³„ ì¸ë±ìŠ¤ ê´€ë¦¬
        self.expression_indices: Dict[str, Dict] = {}
        self.model_embeddings: Dict[str, Dict[str, np.ndarray]] = {}
        
        logger.info(f"Hybrid Vector Store ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {len(self.embedding_models)}ê°œ)")
    
    def add_chunks_with_expressions(self, chunks: List[TextChunk], 
                                  expression_enhancer=None) -> None:
        """
        í‘œí˜„ì„ ê³ ë ¤í•œ ì²­í¬ ì¶”ê°€
        
        Args:
            chunks: í…ìŠ¤íŠ¸ ì²­í¬ë“¤
            expression_enhancer: í‘œí˜„ í–¥ìƒê¸° (KeywordEnhancer)
        """
        # ê¸°ë³¸ ì²­í¬ ì¶”ê°€
        self.faiss_store.add_chunks(chunks)
        self.chroma_store.add_chunks(chunks)
        
        # í‘œí˜„ë³„ ì¸ë±ìŠ¤ ìƒì„±
        if expression_enhancer:
            self._create_expression_indices(chunks, expression_enhancer)
    
    def _create_expression_indices(self, chunks: List[TextChunk], 
                                 expression_enhancer) -> None:
        """í‘œí˜„ë³„ ì¸ë±ìŠ¤ ìƒì„±"""
        for chunk in chunks:
            # êµí†µ, ë°ì´í„°ë² ì´ìŠ¤, ì¼ë°˜ ì»¨í…ìŠ¤íŠ¸ë³„ í‘œí˜„ ì¶”ì¶œ
            for context in ["traffic", "database", "general"]:
                expressions = expression_enhancer.get_multi_expressions(
                    chunk.content, context
                )
                
                if expressions:
                    if context not in self.expression_indices:
                        self.expression_indices[context] = {}
                    
                    for expr in expressions:
                        if expr not in self.expression_indices[context]:
                            self.expression_indices[context][expr] = []
                        self.expression_indices[context][expr].append(chunk.chunk_id)
    
    def search_with_expressions(self, query: str, 
                              expression_enhancer=None,
                              context: str = "general",
                              top_k: int = 5) -> List[Tuple[TextChunk, float]]:
        """
        í‘œí˜„ì„ ê³ ë ¤í•œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            expression_enhancer: í‘œí˜„ í–¥ìƒê¸°
            context: ì»¨í…ìŠ¤íŠ¸
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            (ì²­í¬, ìœ ì‚¬ë„) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        # ê¸°ë³¸ ê²€ìƒ‰
        basic_results = self.search(query, top_k=top_k)
        
        if not expression_enhancer:
            return basic_results
        
        # í‘œí˜„ ê¸°ë°˜ ê²€ìƒ‰
        expression_results = self._search_by_expressions(
            query, expression_enhancer, context, top_k
        )
        
        # ê²°ê³¼ í†µí•© ë° ë­í‚¹
        combined_results = self._combine_search_results(
            basic_results, expression_results, top_k
        )
        
        return combined_results
    
    def _search_by_expressions(self, query: str, 
                             expression_enhancer,
                             context: str,
                             top_k: int) -> List[Tuple[TextChunk, float]]:
        """í‘œí˜„ ê¸°ë°˜ ê²€ìƒ‰"""
        expressions = expression_enhancer.get_multi_expressions(query, context)
        if not expressions:
            return []
        
        # í‘œí˜„ë³„ ê´€ë ¨ ì²­í¬ ìˆ˜ì§‘
        expression_chunks = set()
        for expr in expressions:
            if context in self.expression_indices and expr in self.expression_indices[context]:
                chunk_ids = self.expression_indices[context][expr]
                expression_chunks.update(chunk_ids)
        
        # ê´€ë ¨ ì²­í¬ë“¤ì˜ ì„ë² ë”© ê²€ìƒ‰
        results = []
        for chunk_id in expression_chunks:
            # ChromaDBì—ì„œ ì²­í¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            chunk_info = self.chroma_store.get_chunk_by_id(chunk_id)
            if chunk_info:
                # ì„ì‹œ ìœ ì‚¬ë„ ì ìˆ˜ (í‘œí˜„ ë§¤ì¹­ ê¸°ë°˜)
                similarity = 0.8  # ê¸°ë³¸ ì ìˆ˜
                results.append((chunk_info, similarity))
        
        return sorted(results, key=lambda x: x[1], reverse=True)[:top_k]
    
    def _combine_search_results(self, 
                              basic_results: List[Tuple[TextChunk, float]],
                              expression_results: List[Tuple[TextChunk, float]],
                              top_k: int) -> List[Tuple[TextChunk, float]]:
        """ê²€ìƒ‰ ê²°ê³¼ í†µí•©"""
        # ì²­í¬ IDë³„ë¡œ ìµœê³  ì ìˆ˜ ìœ ì§€
        combined_scores = {}
        
        # ê¸°ë³¸ ê²°ê³¼ ì¶”ê°€
        for chunk, score in basic_results:
            combined_scores[chunk.chunk_id] = score
        
        # í‘œí˜„ ê²°ê³¼ ì¶”ê°€ (ë” ë†’ì€ ì ìˆ˜ë¡œ ì—…ë°ì´íŠ¸)
        for chunk, score in expression_results:
            if chunk.chunk_id in combined_scores:
                combined_scores[chunk.chunk_id] = max(
                    combined_scores[chunk.chunk_id], score
                )
            else:
                combined_scores[chunk.chunk_id] = score
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        sorted_chunks = sorted(
            combined_scores.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        # ê²°ê³¼ êµ¬ì„±
        final_results = []
        for chunk_id, score in sorted_chunks[:top_k]:
            chunk = self.chroma_store.get_chunk_by_id(chunk_id)
            if chunk:
                final_results.append((chunk, score))
        
        return final_results
    
    def get_expression_statistics(self) -> Dict[str, Any]:
        """í‘œí˜„ ì¸ë±ìŠ¤ í†µê³„ ë°˜í™˜"""
        stats = {
            "total_contexts": len(self.expression_indices),
            "context_details": {}
        }
        
        for context, expressions in self.expression_indices.items():
            stats["context_details"][context] = {
                "total_expressions": len(expressions),
                "total_chunks": sum(len(chunk_ids) for chunk_ids in expressions.values()),
                "top_expressions": sorted(
                    expressions.items(), 
                    key=lambda x: len(x[1]), 
                    reverse=True
                )[:5]
            }
        
        return stats
    
    def add_chunks(self, chunks: List[TextChunk]) -> None:
        """ë‘ ì €ì¥ì†Œì— ëª¨ë‘ ì²­í¬ ì¶”ê°€"""
        self.faiss_store.add_chunks(chunks)
        self.chroma_store.add_chunks(chunks)
    
    def search(self, query_embedding: np.ndarray, top_k: int = 5,
               use_metadata_filter: bool = False,
               filter_metadata: Optional[Dict] = None,
               similarity_threshold: float = 0.1) -> List[Tuple[TextChunk, float]]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        
        Args:
            query_embedding: ì¿¼ë¦¬ ì„ë² ë”©
            top_k: ê²°ê³¼ ê°œìˆ˜
            use_metadata_filter: ë©”íƒ€ë°ì´í„° í•„í„° ì‚¬ìš© ì—¬ë¶€
            filter_metadata: í•„í„° ì¡°ê±´
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼
        """
        import time
        search_start = time.time()
        
        if use_metadata_filter and filter_metadata:
            # ë©”íƒ€ë°ì´í„° í•„í„°ê°€ í•„ìš”í•œ ê²½ìš° ChromaDB ì‚¬ìš©
            result = self.chroma_store.search(query_embedding, top_k, filter_metadata)
        else:
            # ë¹ ë¥¸ ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš° FAISS ì‚¬ìš©
            result = self.faiss_store.search(query_embedding, top_k)
        
        # ìœ ì‚¬ë„ ì„ê³„ê°’ í•„í„°ë§ ì¶”ê°€
        filtered_result = [(chunk, score) for chunk, score in result if score >= similarity_threshold]
        
        search_time = time.time() - search_start
        print(f"  ğŸ” ë²¡í„° ê²€ìƒ‰: {search_time:.3f}ì´ˆ (FAISS ì‚¬ìš©, ì„ê³„ê°’: {similarity_threshold})")
        
        return filtered_result
    
    def save(self, path: Optional[str] = None) -> None:
        """ë‘ ì €ì¥ì†Œ ëª¨ë‘ ì €ì¥"""
        save_path = path or "./vector_store"
        
        faiss_path = os.path.join(save_path, "faiss")
        self.faiss_store.save(faiss_path)
        
        # ChromaDBëŠ” ìë™ ì €ì¥
        self.chroma_store.save("")
        
        logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ë²¡í„° ì €ì¥ì†Œ ì €ì¥ ì™„ë£Œ: {save_path}")
    
    def load(self, path: Optional[str] = None) -> None:
        """ì €ì¥ëœ ë°ì´í„° ë¡œë“œ"""
        load_path = path or "./vector_store"
        
        faiss_path = os.path.join(load_path, "faiss")
        if os.path.exists(faiss_path):
            self.faiss_store.load(faiss_path)
        
        # ChromaDBëŠ” ìë™ ë¡œë“œ
        logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì™„ë£Œ: {load_path}")

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def calculate_retrieval_metrics(relevant_chunks: List[str], 
                              retrieved_chunks: List[Tuple[TextChunk, float]],
                              k: int = 5) -> Dict[str, float]:
    """
    ê²€ìƒ‰ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
    
    Args:
        relevant_chunks: ì‹¤ì œ ê´€ë ¨ ìˆëŠ” ì²­í¬ ID ë¦¬ìŠ¤íŠ¸
        retrieved_chunks: ê²€ìƒ‰ëœ ì²­í¬ë“¤
        k: ìƒìœ„ kê°œ ê²°ê³¼ì— ëŒ€í•œ í‰ê°€
        
    Returns:
        ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
    """
    if not relevant_chunks or not retrieved_chunks:
        return {"precision": 0.0, "recall": 0.0, "f1": 0.0, "map": 0.0}
    
    # ìƒìœ„ kê°œ ê²°ê³¼ë§Œ ê³ ë ¤
    top_k_chunks = retrieved_chunks[:k]
    retrieved_ids = [chunk.chunk_id for chunk, _ in top_k_chunks]
    
    # Precision@K
    relevant_retrieved = set(relevant_chunks) & set(retrieved_ids)
    precision = len(relevant_retrieved) / len(retrieved_ids) if retrieved_ids else 0.0
    
    # Recall@K  
    recall = len(relevant_retrieved) / len(relevant_chunks)
    
    # F1 Score
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    
    # Mean Average Precision (MAP)
    ap = 0.0
    relevant_count = 0
    for i, chunk_id in enumerate(retrieved_ids):
        if chunk_id in relevant_chunks:
            relevant_count += 1
            ap += relevant_count / (i + 1)
    
    map_score = ap / len(relevant_chunks) if relevant_chunks else 0.0
    
    return {
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "map": map_score
    }

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ë²¡í„° ì €ì¥ì†Œ ëª¨ë“ˆì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ë°ì´í„°ê°€ ìˆì„ ë•Œ)
    # embedding_dim = 768
    # faiss_store = FAISSVectorStore(embedding_dim)
    # chroma_store = ChromaDBVectorStore()
    # hybrid_store = HybridVectorStore(embedding_dim)
