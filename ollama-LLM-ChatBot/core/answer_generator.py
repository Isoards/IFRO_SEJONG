"""
Î°úÏª¨ LLMÏùÑ ÏÇ¨Ïö©Ìïú ÎãµÎ≥Ä ÏÉùÏÑ± Î™®Îìà (ÏµúÏ†ÅÌôî Î≤ÑÏ†Ñ)

Îπ†Î•∏ ÏùëÎãµÏùÑ ÏúÑÌïú Í∞ÑÏÜåÌôîÎêú ÎãµÎ≥Ä ÏÉùÏÑ±Í∏∞
"""

import os
import time
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import logging

# Î°úÏª¨ LLM ÎùºÏù¥Î∏åÎü¨Î¶¨
try:
    import ollama
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    logging.warning("Ollama ÎùºÏù¥Î∏åÎü¨Î¶¨Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")

from .pdf_processor import TextChunk
from .question_analyzer import AnalyzedQuestion
from .fast_cache import get_question_cache

logger = logging.getLogger(__name__)

class ModelType(Enum):
    """ÏßÄÏõêÌïòÎäî Î™®Îç∏ ÌÉÄÏûÖ"""
    OLLAMA = "ollama"

@dataclass
class GenerationConfig:
    """ÌÖçÏä§Ìä∏ ÏÉùÏÑ± ÏÑ§Ï†ï (Ï†ïÌôïÏÑ± ÏµúÏ†ÅÌôî)"""
    max_length: int = 256  # ÎãµÎ≥Ä Í∏∏Ïù¥ Ï†úÌïú
    temperature: float = 0.1  # Îçî Ï†ïÌôïÌïú ÎãµÎ≥ÄÏùÑ ÏúÑÌï¥ ÎÇÆÏùÄ Í∞í
    top_p: float = 0.7  # Îçî ÏßëÏ§ëÎêú ÎãµÎ≥ÄÏùÑ ÏúÑÌï¥ ÎÇÆÏùÄ Í∞í
    top_k: int = 20  # Îçî ÏßëÏ§ëÎêú ÏÑ†ÌÉùÏùÑ ÏúÑÌï¥ ÎÇÆÏùÄ Í∞í
    repetition_penalty: float = 1.1  # Î∞òÎ≥µ Î∞©ÏßÄ ÌôúÏÑ±Ìôî
    do_sample: bool = True
    num_return_sequences: int = 1

@dataclass
class Answer:
    """ÏÉùÏÑ±Îêú ÎãµÎ≥Ä Îç∞Ïù¥ÌÑ∞ ÌÅ¥ÎûòÏä§"""
    content: str
    confidence_score: float
    used_chunks: List[str]
    generation_time: float
    model_name: str
    metadata: Optional[Dict] = None

class OllamaInterface:
    """Ollama Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ (ÏµúÏ†ÅÌôî)"""
    
    def __init__(self, model_name: str = "qwen2:1.5b", config: GenerationConfig = None):
        self.model_name = model_name
        self.config = config or GenerationConfig()
        self.client = ollama.Client(host=os.getenv('OLLAMA_HOST', 'http://localhost:11434'))
        
    def generate(self, prompt: str) -> str:
        """ÌÖçÏä§Ìä∏ ÏÉùÏÑ± (ÏµúÏ†ÅÌôî)"""
        try:
            response = self.client.generate(
                model=self.model_name,
                prompt=prompt,
                options={
                    'temperature': self.config.temperature,
                    'top_p': self.config.top_p,
                    'top_k': self.config.top_k,
                    'num_predict': self.config.max_length,
                    'repeat_penalty': self.config.repetition_penalty,
                    'num_thread': 8,  # Îçî ÎßéÏùÄ Ïä§Î†àÎìú ÏÇ¨Ïö©
                    'num_gpu': 1,     # GPU Í∞ÄÏÜç
                    'num_ctx': 1024,  # Ïª®ÌÖçÏä§Ìä∏ ÌÅ¨Í∏∞ Ï†úÌïú
                    'num_batch': 512, # Î∞∞Ïπò ÌÅ¨Í∏∞ ÏµúÏ†ÅÌôî
                    'rope_freq_base': 10000,  # RoPE ÏµúÏ†ÅÌôî
                    'rope_freq_scale': 0.5    # RoPE Ïä§ÏºÄÏùºÎßÅ
                }
            )
            return response['response']
        except Exception as e:
            logger.error(f"Ollama ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return "ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ±Ìï† Ïàò ÏóÜÏäµÎãàÎã§."

class AnswerGenerator:
    """ÎãµÎ≥Ä ÏÉùÏÑ±Í∏∞ (ÏµúÏ†ÅÌôî)"""
    
    def __init__(self, model_name: str = "qwen2:1.5b", cache_enabled: bool = True):
        self.model_name = model_name
        self.cache_enabled = cache_enabled
        
        # LLM Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ï¥àÍ∏∞Ìôî
        self.llm = OllamaInterface(model_name)
        
        # Ï∫êÏãú Ï¥àÍ∏∞Ìôî
        self.cache = get_question_cache() if cache_enabled else None
        
        # ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶ø (Ï†ïÌôïÏÑ± ÏµúÏ†ÅÌôî)
        self.prompt_templates = {
            "basic": """Îã§Ïùå Î¨∏ÏÑú ÎÇ¥Ïö©ÏùÑ Í∏∞Î∞òÏúºÎ°ú ÏßàÎ¨∏Ïóê ÎãµÎ≥ÄÌï¥Ï£ºÏÑ∏Ïöî.

Î¨∏ÏÑú ÎÇ¥Ïö©:
{context}

ÏßàÎ¨∏: {question}

ÏßÄÏãúÏÇ¨Ìï≠:
1. ÏúÑ Î¨∏ÏÑú ÎÇ¥Ïö©ÏùÑ ÍººÍººÌûà ÏùΩÍ≥† ÏßàÎ¨∏Í≥º Í¥ÄÎ†®Îêú Î™®Îì† Ï†ïÎ≥¥Î•º Ï∞æÏïÑÎ≥¥ÏÑ∏Ïöî.
2. Î¨∏ÏÑúÏóê Í¥ÄÎ†® ÎÇ¥Ïö©Ïù¥ ÏûàÏúºÎ©¥ Î∞òÎìúÏãú Í∑∏ ÎÇ¥Ïö©ÏùÑ Î∞îÌÉïÏúºÎ°ú ÎãµÎ≥ÄÌïòÏÑ∏Ïöî.
3. ÎãµÎ≥ÄÏùÄ Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ïö©Ï†ÅÏúºÎ°ú ÏûëÏÑ±ÌïòÏÑ∏Ïöî.
4. ÏµúÎåÄ {max_length}Ïûê Ïù¥ÎÇ¥Ïóê ÎãµÎ≥ÄÏùÑ ÏôÑÏÑ±ÌïòÏÑ∏Ïöî.
5. Îã®Í≥ÑÎ≥ÑÎ°ú ÏÑ§Î™ÖÌïòÍ±∞ÎÇò ÏòàÏãúÎ•º Îì§Ïñ¥ ÏÑ§Î™ÖÌïòÏÑ∏Ïöî.
6. Î¨∏ÏÑúÏóê Í¥ÄÎ†® Ï†ïÎ≥¥Í∞Ä Ï†ïÎßê ÏóÜÏùÑ ÎïåÎßå "Î¨∏ÏÑúÏóêÏÑú Ìï¥Îãπ ÎÇ¥Ïö©ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."ÎùºÍ≥† ÎãµÎ≥ÄÌïòÏÑ∏Ïöî.
7. ÌÇ§ÏõåÎìú Îß§Ïπ≠Ïù¥ÎÇò Ïú†ÏÇ¨Ìïú ÌëúÌòÑÎèÑ Í≥†Î†§ÌïòÏó¨ ÎãµÎ≥ÄÌïòÏÑ∏Ïöî.

ÎãµÎ≥Ä:"""
        }
        
        logger.info(f"ÎãµÎ≥Ä ÏÉùÏÑ±Í∏∞ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å: {model_name}")
    
    def load_model(self) -> bool:
        """Î™®Îç∏ Î°úÎìú (OllamaÎäî Ïù¥ÎØ∏ Î°úÎìúÎêòÏñ¥ ÏûàÏùå)"""
        try:
            # OllamaÎäî Ïù¥ÎØ∏ Ïã§Ìñâ Ï§ëÏù¥ÎØÄÎ°ú Î™®Îç∏ Î°úÎìú ÏÑ±Í≥µÏúºÎ°ú Í∞ÑÏ£º
            logger.info(f"Ollama Î™®Îç∏ {self.model_name} Î°úÎìú ÏôÑÎ£å")
            return True
        except Exception as e:
            logger.error(f"Î™®Îç∏ Î°úÎìú Ïã§Ìå®: {e}")
            return False
    
    def unload_model(self):
        """Î™®Îç∏ Ïñ∏Î°úÎìú (OllamaÎäî ÏûêÎèôÏúºÎ°ú Í¥ÄÎ¶¨Îê®)"""
        try:
            logger.info(f"Ollama Î™®Îç∏ {self.model_name} Ïñ∏Î°úÎìú ÏôÑÎ£å")
        except Exception as e:
            logger.error(f"Î™®Îç∏ Ïñ∏Î°úÎìú Ïã§Ìå®: {e}")
    
    def generate_answer(self, 
                       analyzed_question: AnalyzedQuestion,
                       relevant_chunks: List[Tuple[TextChunk, float]],
                       conversation_history: List = None,
                       pdf_id: Optional[str] = None) -> Answer:
        """ÏßàÎ¨∏Ïóê ÎåÄÌïú ÎãµÎ≥Ä ÏÉùÏÑ± (ÏµúÏ†ÅÌôî)"""
        total_start_time = time.time()
        question = analyzed_question.original_question
        
        # 1. Ï∫êÏãú ÌôïÏù∏ (Îπ†Î•∏ ÏùëÎãµ)
        cache_start = time.time()
        if self.cache_enabled and self.cache:
            context_key = str([chunk.chunk_id for chunk, _ in relevant_chunks][:3])
            cached_answer = self.cache.get(question, context_key)
            if cached_answer:
                cache_time = time.time() - cache_start
                print(f"  üöÄ Ï∫êÏãú ÌûàÌä∏: {cache_time:.3f}Ï¥à")
                return cached_answer
        cache_time = time.time() - cache_start
        
        # 2. Ïª®ÌÖçÏä§Ìä∏ Íµ¨ÏÑ± (Îã®ÏàúÌôî)
        context_start = time.time()
        context = self._build_context(relevant_chunks)
        context_time = time.time() - context_start
        print(f"  üìù Ïª®ÌÖçÏä§Ìä∏ Íµ¨ÏÑ±: {context_time:.3f}Ï¥à")
        
        # 3. ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ±
        prompt_start = time.time()
        prompt = self.prompt_templates["basic"].format(
            context=context,
            question=question,
            max_length=self.llm.config.max_length
        )
        prompt_time = time.time() - prompt_start
        print(f"  üìã ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ±: {prompt_time:.3f}Ï¥à")
        
        # 4. ÎãµÎ≥Ä ÏÉùÏÑ± (Í∞ÄÏû• Ïò§Îûò Í±∏Î¶¨Îäî Î∂ÄÎ∂Ñ)
        llm_start = time.time()
        try:
            generated_text = self.llm.generate(prompt)
            llm_time = time.time() - llm_start
            print(f"  ü§ñ LLM Ï∂îÎ°†: {llm_time:.2f}Ï¥à")
            
            # 5. Í∏∞Î≥∏ ÌõÑÏ≤òÎ¶¨
            postprocess_start = time.time()
            processed_answer = generated_text.strip()
            if not processed_answer:
                processed_answer = "ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ±Ìï† Ïàò ÏóÜÏäµÎãàÎã§."
            postprocess_time = time.time() - postprocess_start
            print(f"  ‚úÇÔ∏è  ÌõÑÏ≤òÎ¶¨: {postprocess_time:.3f}Ï¥à")
            
            total_time = time.time() - total_start_time
            
            answer = Answer(
                content=processed_answer,
                confidence_score=0.8,  # Í≥†Ï†ï Ïã†Î¢∞ÎèÑ
                used_chunks=[chunk.chunk_id for chunk, _ in relevant_chunks],
                generation_time=total_time,
                model_name=self.llm.model_name,
                metadata={
                    "question_type": analyzed_question.question_type.value,
                    "num_chunks_used": len(relevant_chunks),
                    "from_cache": False,
                    "timing_breakdown": {
                        "cache_check": cache_time,
                        "context_build": context_time,
                        "prompt_build": prompt_time,
                        "llm_generation": llm_time,
                        "postprocess": postprocess_time
                    }
                }
            )
            
            # 6. Ï∫êÏãúÏóê Ï†ÄÏû•
            cache_save_start = time.time()
            if self.cache_enabled and self.cache:
                self.cache.put(question, answer, context_key)
            cache_save_time = time.time() - cache_save_start
            print(f"  üíæ Ï∫êÏãú Ï†ÄÏû•: {cache_save_time:.3f}Ï¥à")
            
            print(f"  üìä LLM ÏÉùÏÑ± ÏÑ∏Î∂Ä: Ï∫êÏãú({cache_time/total_time*100:.1f}%) | Ïª®ÌÖçÏä§Ìä∏({context_time/total_time*100:.1f}%) | ÌîÑÎ°¨ÌîÑÌä∏({prompt_time/total_time*100:.1f}%) | LLM({llm_time/total_time*100:.1f}%) | ÌõÑÏ≤òÎ¶¨({postprocess_time/total_time*100:.1f}%)")
            
            logger.info(f"ÎãµÎ≥Ä ÏÉùÏÑ± ÏôÑÎ£å: {total_time:.2f}Ï¥à")
            return answer
            
        except Exception as e:
            llm_time = time.time() - llm_start
            total_time = time.time() - total_start_time
            logger.error(f"ÎãµÎ≥Ä ÏÉùÏÑ± Ïã§Ìå®: {e}")
            print(f"  ‚ùå LLM Ïò§Î•ò: {llm_time:.2f}Ï¥à ÌõÑ Ïã§Ìå®")
            return Answer(
                content="Ï£ÑÏÜ°Ìï©ÎãàÎã§. ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ±ÌïòÎäî Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.",
                confidence_score=0.0,
                used_chunks=[],
                generation_time=total_time,
                model_name=self.llm.model_name,
                metadata={"error": str(e)}
            )
    
    def _build_context(self, relevant_chunks: List[Tuple[TextChunk, float]], 
                       max_context_length: int = 800) -> str:
        """Ïª®ÌÖçÏä§Ìä∏ Íµ¨ÏÑ± (ÏµúÏ†ÅÌôî)"""
        if not relevant_chunks:
            return "Í¥ÄÎ†® Ï†ïÎ≥¥Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."
        
        context_parts = []
        current_length = 0
        
        for chunk, similarity in relevant_chunks:
            chunk_text = chunk.content.strip()
            if current_length + len(chunk_text) > max_context_length:
                break
            
            context_parts.append(f"[Ïú†ÏÇ¨ÎèÑ: {similarity:.2f}] {chunk_text}")
            current_length += len(chunk_text)
        
        return "\n\n".join(context_parts)
    
    def update_model_config(self, config: GenerationConfig):
        """Î™®Îç∏ ÏÑ§Ï†ï ÏóÖÎç∞Ïù¥Ìä∏"""
        self.llm.config = config
        logger.info("Î™®Îç∏ ÏÑ§Ï†ï ÏóÖÎç∞Ïù¥Ìä∏ ÏôÑÎ£å")
    
    def get_model_info(self) -> Dict:
        """Î™®Îç∏ Ï†ïÎ≥¥ Î∞òÌôò"""
        return {
            "model_name": self.model_name,
            "model_type": ModelType.OLLAMA.value,
            "cache_enabled": self.cache_enabled,
            "config": {
                "max_length": self.llm.config.max_length,
                "temperature": self.llm.config.temperature,
                "top_p": self.llm.config.top_p
            }
        }
