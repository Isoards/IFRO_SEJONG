#!/usr/bin/env python3
"""
ì¸ì‚¬ë§ ì²˜ë¦¬ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from core.question_analyzer import QuestionAnalyzer
from core.answer_generator import AnswerGenerator, ModelType
from core.pdf_processor import TextChunk

def test_greeting_detection():
    """ì¸ì‚¬ë§ ê°ì§€ í…ŒìŠ¤íŠ¸"""
    print("=" * 50)
    print("ì¸ì‚¬ë§ ê°ì§€ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    analyzer = QuestionAnalyzer()
    
    # í…ŒìŠ¤íŠ¸í•  ì¸ì‚¬ë§ë“¤
    test_greetings = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "ì•ˆë…•",
        "ë°˜ê°‘ìŠµë‹ˆë‹¤",
        "ë°˜ê°€ì›Œìš”",
        "í•˜ì´",
        "hi",
        "hello",
        "ì¢‹ì€ ì•„ì¹¨ì…ë‹ˆë‹¤",
        "ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”",
        "ë§Œë‚˜ì„œ ë°˜ê°€ì›Œ",
        "ì˜¤ëœë§Œì´ë„¤ìš”",
        "ì˜ ì§€ë‚´ì„¸ìš”?",
        "ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”?",
        "í•˜ì´í•˜ì´",
        "í—¬ë¡œìš°"
    ]
    
    # í…ŒìŠ¤íŠ¸í•  ì¼ë°˜ ì§ˆë¬¸ë“¤
    test_questions = [
        "êµí†µëŸ‰ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "ì‚¬ê³  ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
        "ì´ ë¬¸ì„œì˜ ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”"
    ]
    
    print("\nì¸ì‚¬ë§ í…ŒìŠ¤íŠ¸:")
    for greeting in test_greetings:
        analyzed = analyzer.analyze_question(greeting)
        is_greeting = analyzed.question_type.value == "greeting"
        print(f"'{greeting}' -> {'ì¸ì‚¬ë§' if is_greeting else 'ì¼ë°˜ì§ˆë¬¸'}")
    
    print("\nì¼ë°˜ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸:")
    for question in test_questions:
        analyzed = analyzer.analyze_question(question)
        is_greeting = analyzed.question_type.value == "greeting"
        print(f"'{question}' -> {'ì¸ì‚¬ë§' if is_greeting else 'ì¼ë°˜ì§ˆë¬¸'}")

def test_greeting_response():
    """ì¸ì‚¬ë§ ì‘ë‹µ ìƒì„± í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 50)
    print("ì¸ì‚¬ë§ ì‘ë‹µ ìƒì„± í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # ê°„ë‹¨í•œ ëª¨ì˜ LLM í´ë˜ìŠ¤
    class MockLLM:
        def __init__(self):
            self.model_name = "mock_model"
        
        def generate(self, prompt):
            # í”„ë¡¬í”„íŠ¸ì—ì„œ ì¸ì‚¬ë§ ì‘ë‹µ ìƒì„±
            if "ì¸ì‚¬í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸" in prompt:
                responses = [
                    "ì•ˆë…•í•˜ì„¸ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤! ğŸ˜Š ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
                    "í•˜ì´í•˜ì´! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”! âœ¨ ì˜¤ëŠ˜ë„ ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”!",
                    "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” IFRO ì±—ë´‡ì…ë‹ˆë‹¤. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”! ğŸŒŸ",
                    "ë°˜ê°‘ìŠµë‹ˆë‹¤! êµí†µ ë°ì´í„° ë¶„ì„ì´ë‚˜ ëŒ€ì‹œë³´ë“œ ì‚¬ìš©ë²•ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”! ğŸš—"
                ]
                import random
                return random.choice(responses)
            else:
                return "ì¼ë°˜ì ì¸ ë‹µë³€ì…ë‹ˆë‹¤."
    
    # ë‹µë³€ ìƒì„±ê¸° ì´ˆê¸°í™”
    answer_generator = AnswerGenerator(
        model_type=ModelType.OLLAMA,  # ì„ì‹œë¡œ OLLAMA ì‚¬ìš©
        model_name="mock_model",
        generation_config=None
    )
    answer_generator.llm = MockLLM()
    
    # í…ŒìŠ¤íŠ¸í•  ì¸ì‚¬ë§ë“¤
    test_greetings = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "ë°˜ê°‘ìŠµë‹ˆë‹¤",
        "í•˜ì´",
        "ì¢‹ì€ ì•„ì¹¨ì…ë‹ˆë‹¤"
    ]
    
    analyzer = QuestionAnalyzer()
    
    for greeting in test_greetings:
        print(f"\nì‚¬ìš©ì: {greeting}")
        
        # ì§ˆë¬¸ ë¶„ì„
        analyzed = analyzer.analyze_question(greeting)
        print(f"ë¶„ì„ ê²°ê³¼: {analyzed.question_type.value}")
        
        # ë‹µë³€ ìƒì„±
        try:
            answer = answer_generator.generate_answer(
                analyzed_question=analyzed,
                relevant_chunks=[],  # ì¸ì‚¬ë§ì€ ì»¨í…ìŠ¤íŠ¸ ë¶ˆí•„ìš”
                conversation_history=None
            )
            print(f"ì±—ë´‡: {answer.content}")
        except Exception as e:
            print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    test_greeting_detection()
    test_greeting_response()
    
    print("\n" + "=" * 50)
    print("í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 50)
