"""
TypeScript/Django ì—°ë™ì„ ìœ„í•œ FastAPI ì—”ë“œí¬ì¸íŠ¸

ì´ ëª¨ë“ˆì€ PDF QA ì‹œìŠ¤í…œì˜ ëª¨ë“  ê¸°ëŠ¥ì„ REST APIë¡œ ì œê³µí•˜ì—¬
TypeScript í”„ë¡ íŠ¸ì—”ë“œì™€ Django ë°±ì—”ë“œì—ì„œ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
"""

import os
import uuid
import tempfile
from typing import List, Dict, Optional, Any, Union
from datetime import datetime
import logging

from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
import uvicorn

# í•µì‹¬ ëª¨ë“ˆë“¤ ì„í¬íŠ¸
from core.pdf_processor import PDFProcessor, TextChunk
from core.vector_store import HybridVectorStore, VectorStoreInterface
from core.question_analyzer import QuestionAnalyzer, AnalyzedQuestion, ConversationItem
from core.answer_generator import AnswerGenerator, Answer, ModelType, GenerationConfig
from core.evaluator import PDFQAEvaluator, SystemEvaluation
from core.sql_generator import SQLGenerator, DatabaseSchema, SQLQuery
from core.dual_pipeline_processor import DualPipelineProcessor, DualPipelineResult

logger = logging.getLogger(__name__)

# Pydantic ëª¨ë¸ë“¤ (API ìŠ¤í‚¤ë§ˆ ì •ì˜)
class QuestionRequest(BaseModel):
    """ì§ˆë¬¸ ìš”ì²­ ëª¨ë¸"""
    question: str = Field(..., description="ì‚¬ìš©ì ì§ˆë¬¸")
    pdf_id: str = Field(..., description="PDF ë¬¸ì„œ ì‹ë³„ì")
    use_conversation_context: bool = Field(True, description="ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš© ì—¬ë¶€")
    max_chunks: int = Field(5, description="ê²€ìƒ‰í•  ìµœëŒ€ ì²­í¬ ìˆ˜")
    use_dual_pipeline: bool = Field(True, description="Dual Pipeline ì‚¬ìš© ì—¬ë¶€")
    
class ConversationHistoryItem(BaseModel):
    """ëŒ€í™” ê¸°ë¡ í•­ëª© ëª¨ë¸"""
    question: str
    answer: str
    timestamp: str
    confidence_score: float = 0.0

class QuestionResponse(BaseModel):
    """ì§ˆë¬¸ ì‘ë‹µ ëª¨ë¸"""
    answer: str = Field(..., description="ìƒì„±ëœ ë‹µë³€")
    confidence_score: float = Field(..., description="ë‹µë³€ ì‹ ë¢°ë„ (0-1)")
    used_chunks: List[str] = Field(..., description="ì‚¬ìš©ëœ ë¬¸ì„œ ì²­í¬ IDë“¤")
    generation_time: float = Field(..., description="ë‹µë³€ ìƒì„± ì‹œê°„ (ì´ˆ)")
    question_type: str = Field(..., description="ì§ˆë¬¸ ìœ í˜•")
    model_name: str = Field(..., description="ì‚¬ìš©ëœ ëª¨ë¸ ì´ë¦„")
    pipeline_type: str = Field(..., description="ì‚¬ìš©ëœ íŒŒì´í”„ë¼ì¸ íƒ€ì…")
    sql_query: Optional[str] = Field(None, description="ìƒì„±ëœ SQL ì¿¼ë¦¬ (ìˆëŠ” ê²½ìš°)")
    
class PDFUploadResponse(BaseModel):
    """PDF ì—…ë¡œë“œ ì‘ë‹µ ëª¨ë¸"""
    pdf_id: str = Field(..., description="ìƒì„±ëœ PDF ì‹ë³„ì")
    filename: str = Field(..., description="ì—…ë¡œë“œëœ íŒŒì¼ëª…")
    total_pages: int = Field(..., description="ì´ í˜ì´ì§€ ìˆ˜")
    total_chunks: int = Field(..., description="ìƒì„±ëœ ì²­í¬ ìˆ˜")
    processing_time: float = Field(..., description="ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)")
    
class EvaluationRequest(BaseModel):
    """í‰ê°€ ìš”ì²­ ëª¨ë¸"""
    questions: List[str]
    generated_answers: List[str]
    reference_answers: List[str]
    
class ModelConfigRequest(BaseModel):
    """ëª¨ë¸ ì„¤ì • ìš”ì²­ ëª¨ë¸"""
    model_type: str = Field("ollama", description="ëª¨ë¸ íƒ€ì… (ollama/huggingface/llama_cpp)")
    model_name: str = Field("llama2:7b", description="ëª¨ë¸ ì´ë¦„")
    max_length: int = Field(512, description="ìµœëŒ€ ìƒì„± ê¸¸ì´")
    temperature: float = Field(0.7, description="ìƒì„± ì˜¨ë„")
    top_p: float = Field(0.9, description="Top-p ìƒ˜í”Œë§")

class SystemStatusResponse(BaseModel):
    """ì‹œìŠ¤í…œ ìƒíƒœ ì‘ë‹µ ëª¨ë¸"""
    status: str = Field(..., description="ì‹œìŠ¤í…œ ìƒíƒœ")
    model_loaded: bool = Field(..., description="ëª¨ë¸ ë¡œë“œ ìƒíƒœ")
    total_pdfs: int = Field(..., description="ë“±ë¡ëœ PDF ìˆ˜")
    total_chunks: int = Field(..., description="ì´ ì²­í¬ ìˆ˜")
    memory_usage: Dict[str, Any] = Field(..., description="ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰")

# ì „ì—­ ê°ì²´ë“¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
pdf_processor: Optional[PDFProcessor] = None
vector_store: Optional[VectorStoreInterface] = None
question_analyzer: Optional[QuestionAnalyzer] = None
answer_generator: Optional[AnswerGenerator] = None
evaluator: Optional[PDFQAEvaluator] = None
sql_generator: Optional[SQLGenerator] = None
dual_pipeline_processor: Optional[DualPipelineProcessor] = None

# PDF ë©”íƒ€ë°ì´í„° ì €ì¥ì†Œ (ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš© ê¶Œì¥)
pdf_metadata: Dict[str, Dict] = {}

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI(
    title="PDF QA System API",
    description="ë¡œì»¬ LLMì„ ì‚¬ìš©í•˜ëŠ” PDF ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€ ì‹œìŠ¤í…œ API",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS ì„¤ì • (TypeScript í”„ë¡ íŠ¸ì—”ë“œ ì§€ì›)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:8080", "http://127.0.0.1:3000", "http://127.0.0.1:8000"],  # React/Django ê°œë°œ ì„œë²„
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì˜ì¡´ì„± í•¨ìˆ˜ë“¤
def get_pdf_processor() -> PDFProcessor:
    """PDF ì²˜ë¦¬ê¸° ì˜ì¡´ì„±"""
    global pdf_processor
    if pdf_processor is None:
        pdf_processor = PDFProcessor()
    return pdf_processor

def get_vector_store() -> VectorStoreInterface:
    """ë²¡í„° ì €ì¥ì†Œ ì˜ì¡´ì„±"""
    global vector_store
    if vector_store is None:
        vector_store = HybridVectorStore()
    return vector_store

def get_question_analyzer() -> QuestionAnalyzer:
    """ì§ˆë¬¸ ë¶„ì„ê¸° ì˜ì¡´ì„±"""
    global question_analyzer
    if question_analyzer is None:
        question_analyzer = QuestionAnalyzer()
    return question_analyzer

def get_answer_generator() -> AnswerGenerator:
    """ë‹µë³€ ìƒì„±ê¸° ì˜ì¡´ì„±"""
    global answer_generator
    if answer_generator is None:
        answer_generator = AnswerGenerator()
        if not answer_generator.load_model():
            logger.error("ë‹µë³€ ìƒì„± ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
            # ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œì—ë„ ê¸°ë³¸ ê°ì²´ëŠ” ë°˜í™˜í•˜ë˜, ì‹¤ì œ ì‚¬ìš© ì‹œ ì˜¤ë¥˜ ì²˜ë¦¬
            answer_generator.is_loaded = False
    return answer_generator

def get_evaluator() -> PDFQAEvaluator:
    """í‰ê°€ê¸° ì˜ì¡´ì„±"""
    global evaluator
    if evaluator is None:
        evaluator = PDFQAEvaluator()
    return evaluator

def get_sql_generator() -> SQLGenerator:
    """SQL ìƒì„±ê¸° ì˜ì¡´ì„±"""
    global sql_generator
    if sql_generator is None:
        sql_generator = SQLGenerator()
    return sql_generator

def get_dual_pipeline_processor() -> DualPipelineProcessor:
    """Dual Pipeline ì²˜ë¦¬ê¸° ì˜ì¡´ì„±"""
    global dual_pipeline_processor
    if dual_pipeline_processor is None:
        # í•„ìš”í•œ ì»´í¬ë„ŒíŠ¸ë“¤ ê°€ì ¸ì˜¤ê¸°
        qa = get_question_analyzer()
        ag = get_answer_generator()
        sg = get_sql_generator()
        vs = get_vector_store()
        
        # ìƒ˜í”Œ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ (ì‹¤ì œë¡œëŠ” ì„¤ì •ì—ì„œ ë¡œë“œ)
        sample_schema = DatabaseSchema(
            table_name="users",
            columns=[
                {"name": "id", "type": "INTEGER", "description": "ì‚¬ìš©ì ID"},
                {"name": "name", "type": "TEXT", "description": "ì‚¬ìš©ì ì´ë¦„"},
                {"name": "email", "type": "TEXT", "description": "ì´ë©”ì¼"},
                {"name": "created_at", "type": "DATETIME", "description": "ê°€ì…ì¼"},
                {"name": "status", "type": "TEXT", "description": "ìƒíƒœ"}
            ],
            primary_key="id",
            sample_data=[
                {"id": 1, "name": "ê¹€ì² ìˆ˜", "email": "kim@example.com", "created_at": "2023-01-01", "status": "active"},
                {"id": 2, "name": "ì´ì˜í¬", "email": "lee@example.com", "created_at": "2023-02-15", "status": "active"}
            ]
        )
        
        dual_pipeline_processor = DualPipelineProcessor(
            question_analyzer=qa,
            answer_generator=ag,
            sql_generator=sg,
            vector_store=vs,
            database_schema=sample_schema
        )
    return dual_pipeline_processor

# API ì—”ë“œí¬ì¸íŠ¸ë“¤

@app.get("/", response_model=Dict[str, str])
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "PDF QA System API",
        "version": "1.0.0",
        "docs": "/docs"
    }

@app.get("/status", response_model=SystemStatusResponse)
async def get_system_status(
    vector_store: VectorStoreInterface = Depends(get_vector_store),
    answer_generator: AnswerGenerator = Depends(get_answer_generator)
):
    """ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
    import psutil
    import gc
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ
    process = psutil.Process()
    memory_info = process.memory_info()
    
    return SystemStatusResponse(
        status="running",
        model_loaded=answer_generator.llm.is_loaded,
        total_pdfs=len(pdf_metadata),
        total_chunks=sum(meta.get("total_chunks", 0) for meta in pdf_metadata.values()),
        memory_usage={
            "rss_mb": round(memory_info.rss / 1024 / 1024, 2),
            "vms_mb": round(memory_info.vms / 1024 / 1024, 2),
            "cpu_percent": psutil.cpu_percent()
        }
    )

@app.post("/upload_pdf", response_model=PDFUploadResponse)
async def upload_pdf(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    pdf_processor: PDFProcessor = Depends(get_pdf_processor),
    vector_store: VectorStoreInterface = Depends(get_vector_store)
):
    """PDF íŒŒì¼ ì—…ë¡œë“œ ë° ì²˜ë¦¬"""
    import time
    
    start_time = time.time()
    
    # íŒŒì¼ ê²€ì¦
    if not file.filename.lower().endswith('.pdf'):
        raise HTTPException(status_code=400, detail="PDF íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    
    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    pdf_id = str(uuid.uuid4())
    
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:
            content = await file.read()
            temp_file.write(content)
            temp_path = temp_file.name
        
        # PDF ì²˜ë¦¬
        chunks, metadata = pdf_processor.process_pdf(temp_path)
        
        # ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€
        vector_store.add_chunks(chunks)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        pdf_metadata[pdf_id] = {
            "filename": file.filename,
            "upload_time": datetime.now().isoformat(),
            "total_pages": metadata.get("pages", 0),
            "total_chunks": len(chunks),
            "extraction_method": metadata.get("extraction_method", [])
        }
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë²¡í„° ì €ì¥ì†Œ ì €ì¥
        background_tasks.add_task(vector_store.save)
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.unlink(temp_path)
        
        processing_time = time.time() - start_time
        
        logger.info(f"PDF ì—…ë¡œë“œ ì™„ë£Œ: {file.filename} ({len(chunks)}ê°œ ì²­í¬)")
        
        return PDFUploadResponse(
            pdf_id=pdf_id,
            filename=file.filename,
            total_pages=metadata.get("pages", 0),
            total_chunks=len(chunks),
            processing_time=processing_time
        )
        
    except Exception as e:
        logger.error(f"PDF ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.post("/ask", response_model=QuestionResponse)
async def ask_question(
    request: QuestionRequest,
    vector_store: VectorStoreInterface = Depends(get_vector_store),
    question_analyzer: QuestionAnalyzer = Depends(get_question_analyzer),
    answer_generator: AnswerGenerator = Depends(get_answer_generator),
    dual_pipeline_processor: DualPipelineProcessor = Depends(get_dual_pipeline_processor)
):
    """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (Dual Pipeline ì§€ì›)"""
    
    # PDF ì¡´ì¬ í™•ì¸
    if request.pdf_id not in pdf_metadata:
        raise HTTPException(status_code=404, detail="PDFë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    try:
        # Dual Pipeline ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ì²˜ë¦¬ ë°©ì‹ ë¶„ê¸°
        if request.use_dual_pipeline and dual_pipeline_processor:
            try:
                # Dual Pipeline ì²˜ë¦¬
                result = dual_pipeline_processor.process_question(
                    question=request.question,
                    use_context=request.use_conversation_context
                )
                
                # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
                question_analyzer.add_conversation_item(
                    request.question,
                    result.final_answer,
                    [],  # Dual Pipelineì—ì„œëŠ” ì²­í¬ IDë¥¼ ë³„ë„ë¡œ ê´€ë¦¬í•˜ì§€ ì•ŠìŒ
                    result.confidence_score
                )
                
                # SQL ì¿¼ë¦¬ ì •ë³´ ì¶”ì¶œ
                sql_query = None
                if result.sql_result and result.sql_result.sql_query:
                    sql_query = result.sql_result.sql_query.query
                
                return QuestionResponse(
                    answer=result.final_answer,
                    confidence_score=result.confidence_score,
                    used_chunks=[],  # Dual Pipelineì—ì„œëŠ” ì²­í¬ IDë¥¼ ë³„ë„ë¡œ ê´€ë¦¬í•˜ì§€ ì•ŠìŒ
                    generation_time=result.total_processing_time,
                    question_type=result.analyzed_question.question_type.value,
                    model_name="dual_pipeline",
                    pipeline_type=result.metadata.get("pipeline_type", "unknown"),
                    sql_query=sql_query
                )
            except Exception as dual_error:
                logger.error(f"Dual Pipeline ì²˜ë¦¬ ì‹¤íŒ¨: {dual_error}")
                # Dual Pipeline ì‹¤íŒ¨ ì‹œ ì¼ë°˜ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ í´ë°±
                logger.info("ì¼ë°˜ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
                request.use_dual_pipeline = False
        
        else:
            # ê¸°ì¡´ ë‹¨ì¼ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬
            # 1. ì§ˆë¬¸ ë¶„ì„
            analyzed_question = question_analyzer.analyze_question(
                request.question,
                use_conversation_context=request.use_conversation_context
            )
            
            # 2. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            query_embedding = analyzed_question.embedding
            relevant_chunks = vector_store.search(
                query_embedding,
                top_k=request.max_chunks
            )
            
            if not relevant_chunks:
                raise HTTPException(status_code=404, detail="ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # 3. ì´ì „ ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
            conversation_history = None
            if request.use_conversation_context:
                conversation_history = question_analyzer.get_conversation_context(max_items=3)
            
            # 4. ë‹µë³€ ìƒì„± (ëŒ€í™” ì´ë ¥ ìºì‹œ í¬í•¨)
            if not answer_generator.is_loaded:
                # ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ ë‹µë³€ ìƒì„±
                answer_content = "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ AI ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
                confidence_score = 0.0
                used_chunks = [chunk.chunk_id for chunk in relevant_chunks]
                generation_time = 0.0
                model_name = "error"
            else:
                answer = answer_generator.generate_answer(
                    analyzed_question,
                    relevant_chunks,
                    conversation_history,
                    pdf_id=request.pdf_id
                )
                answer_content = answer.content
                confidence_score = answer.confidence_score
                used_chunks = answer.used_chunks
                generation_time = answer.generation_time
                model_name = answer.model_name
            
            # 5. ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
            question_analyzer.add_conversation_item(
                request.question,
                answer_content,
                used_chunks,
                confidence_score
            )
            
            # 6. ìºì‹œ ì‚¬ìš© ì—¬ë¶€ í™•ì¸
            from_cache = False
            if answer_generator.is_loaded and hasattr(answer, 'metadata') and answer.metadata:
                from_cache = answer.metadata.get("from_cache", False)
            
            logger.info(f"ì§ˆë¬¸ ì²˜ë¦¬ ì™„ë£Œ: {analyzed_question.question_type.value}, ìºì‹œ: {from_cache}")
            
            return QuestionResponse(
                answer=answer_content,
                confidence_score=confidence_score,
                used_chunks=used_chunks,
                generation_time=generation_time,
                question_type=analyzed_question.question_type.value,
                model_name=model_name,
                pipeline_type="document_search",
                sql_query=None
            )
        
    except Exception as e:
        logger.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        import traceback
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜
        return QuestionResponse(
            answer="ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì„œë²„ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”.",
            confidence_score=0.0,
            used_chunks=[],
            generation_time=0.0,
            question_type="error",
            model_name="error",
            pipeline_type="error",
            sql_query=None
        )

@app.get("/conversation_history")
async def get_conversation_history(
    pdf_id: str,
    max_items: int = 10,
    question_analyzer: QuestionAnalyzer = Depends(get_question_analyzer)
):
    """ëŒ€í™” ê¸°ë¡ ì¡°íšŒ"""
    if pdf_id not in pdf_metadata:
        raise HTTPException(status_code=404, detail="PDFë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    history = question_analyzer.get_conversation_context(max_items=max_items)
    return {"conversation_history": history}

@app.delete("/conversation_history")
async def clear_conversation_history(
    question_analyzer: QuestionAnalyzer = Depends(get_question_analyzer)
):
    """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
    question_analyzer.conversation_history.clear()
    return {"message": "ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤."}

@app.post("/configure_model")
async def configure_model(request: ModelConfigRequest):
    """ëª¨ë¸ ì„¤ì • ë³€ê²½"""
    global answer_generator
    
    try:
        # ê¸°ì¡´ ëª¨ë¸ ì–¸ë¡œë“œ
        if answer_generator:
            answer_generator.unload_model()
        
        # ìƒˆ ì„¤ì •ìœ¼ë¡œ ëª¨ë¸ ì´ˆê¸°í™”
        config = GenerationConfig(
            max_length=request.max_length,
            temperature=request.temperature,
            top_p=request.top_p
        )
        
        model_type = ModelType(request.model_type)
        answer_generator = AnswerGenerator(
            model_type=model_type,
            model_name=request.model_name,
            generation_config=config
        )
        
        # ëª¨ë¸ ë¡œë“œ
        if not answer_generator.load_model():
            raise HTTPException(status_code=500, detail="ìƒˆ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
        
        return {"message": f"ëª¨ë¸ ì„¤ì •ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤: {request.model_name}"}
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ì„¤ì • ë³€ê²½ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ëª¨ë¸ ì„¤ì • ë³€ê²½ ì‹¤íŒ¨: {str(e)}")

@app.post("/evaluate")
async def evaluate_system(
    request: EvaluationRequest,
    evaluator: PDFQAEvaluator = Depends(get_evaluator)
):
    """ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€"""
    try:
        # ê°„ë‹¨í•œ í‰ê°€ ì‹¤í–‰ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ í‰ê°€ í•„ìš”)
        semantic_similarities = []
        
        for gen_answer, ref_answer in zip(request.generated_answers, request.reference_answers):
            similarity = evaluator._calculate_semantic_similarity(gen_answer, ref_answer)
            semantic_similarities.append(similarity)
        
        avg_similarity = sum(semantic_similarities) / len(semantic_similarities)
        
        return {
            "evaluation_results": {
                "average_semantic_similarity": avg_similarity,
                "individual_similarities": semantic_similarities,
                "total_questions": len(request.questions)
            }
        }
        
    except Exception as e:
        logger.error(f"í‰ê°€ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/pdfs")
async def list_pdfs():
    """ë“±ë¡ëœ PDF ëª©ë¡ ì¡°íšŒ"""
    pdf_list = []
    for pdf_id, metadata in pdf_metadata.items():
        pdf_list.append({
            "pdf_id": pdf_id,
            "filename": metadata["filename"],
            "upload_time": metadata["upload_time"],
            "total_pages": metadata["total_pages"],
            "total_chunks": metadata["total_chunks"]
        })
    
    return {"pdfs": pdf_list}

@app.post("/load_existing_pdfs")
async def load_existing_pdfs(
    background_tasks: BackgroundTasks,
    pdf_processor: PDFProcessor = Depends(get_pdf_processor),
    vector_store: VectorStoreInterface = Depends(get_vector_store)
):
    """ê¸°ì¡´ PDF íŒŒì¼ë“¤ì„ ìë™ìœ¼ë¡œ ë¡œë“œ"""
    import glob
    from pathlib import Path
    
    pdf_dir = Path("data/pdfs")
    if not pdf_dir.exists():
        raise HTTPException(status_code=404, detail="data/pdfs í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ëª¨ë“  PDF íŒŒì¼ ì°¾ê¸°
    pdf_files = []
    for pattern in ["*.pdf", "*/*.pdf", "*/*/*.pdf"]:
        pdf_files.extend(pdf_dir.glob(pattern))
    
    if not pdf_files:
        raise HTTPException(status_code=404, detail="PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    loaded_pdfs = []
    
    for pdf_path in pdf_files:
        try:
            # ì´ë¯¸ ì²˜ë¦¬ëœ PDFì¸ì§€ í™•ì¸
            pdf_id = str(uuid.uuid4())
            
            # PDF ì²˜ë¦¬
            chunks, metadata = pdf_processor.process_pdf(str(pdf_path))
            
            # ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€
            vector_store.add_chunks(chunks)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            pdf_metadata[pdf_id] = {
                "filename": pdf_path.name,
                "file_path": str(pdf_path),
                "upload_time": datetime.now().isoformat(),
                "total_pages": metadata.get("pages", 0),
                "total_chunks": len(chunks),
                "extraction_method": metadata.get("extraction_method", [])
            }
            
            loaded_pdfs.append({
                "pdf_id": pdf_id,
                "filename": pdf_path.name,
                "file_path": str(pdf_path),
                "total_pages": metadata.get("pages", 0),
                "total_chunks": len(chunks)
            })
            
            logger.info(f"ê¸°ì¡´ PDF ë¡œë“œ ì™„ë£Œ: {pdf_path.name} ({len(chunks)}ê°œ ì²­í¬)")
            
        except Exception as e:
            logger.error(f"PDF ë¡œë“œ ì‹¤íŒ¨ {pdf_path}: {e}")
            continue
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë²¡í„° ì €ì¥ì†Œ ì €ì¥
    background_tasks.add_task(vector_store.save)
    
    return {
        "message": f"{len(loaded_pdfs)}ê°œì˜ PDF íŒŒì¼ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.",
        "loaded_pdfs": loaded_pdfs
    }

@app.delete("/pdfs/{pdf_id}")
async def delete_pdf(pdf_id: str):
    """PDF ì‚­ì œ"""
    if pdf_id not in pdf_metadata:
        raise HTTPException(status_code=404, detail="PDFë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ë©”íƒ€ë°ì´í„°ì—ì„œ ì œê±°
    del pdf_metadata[pdf_id]
    
    # ì‹¤ì œë¡œëŠ” ë²¡í„° ì €ì¥ì†Œì—ì„œë„ í•´ë‹¹ ì²­í¬ë“¤ì„ ì œê±°í•´ì•¼ í•¨
    # í˜„ì¬ êµ¬í˜„ì—ì„œëŠ” ì „ì²´ ë²¡í„° ì €ì¥ì†Œ ì¬êµ¬ì„±ì´ í•„ìš”
    
    return {"message": f"PDF {pdf_id}ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."}

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

# ëŒ€í™” ì´ë ¥ ê´€ë¦¬ ì—”ë“œí¬ì¸íŠ¸ë“¤

@app.get("/conversation/cache/stats")
async def get_conversation_cache_stats(
    answer_generator: AnswerGenerator = Depends(get_answer_generator)
):
    """ëŒ€í™” ì´ë ¥ ìºì‹œ í†µê³„ ì¡°íšŒ"""
    try:
        stats = answer_generator.get_conversation_statistics()
        return stats
    except Exception as e:
        logger.error(f"ëŒ€í™” ì´ë ¥ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.delete("/conversation/cache")
async def clear_conversation_cache(
    pdf_id: Optional[str] = None,
    answer_generator: AnswerGenerator = Depends(get_answer_generator)
):
    """ëŒ€í™” ì´ë ¥ ìºì‹œ ì‚­ì œ"""
    try:
        deleted_count = answer_generator.clear_conversation_cache(pdf_id)
        return {
            "message": f"ëŒ€í™” ì´ë ¥ ìºì‹œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "deleted_count": deleted_count,
            "pdf_id": pdf_id
        }
    except Exception as e:
        logger.error(f"ëŒ€í™” ì´ë ¥ ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")

@app.get("/conversation/cache/search")
async def search_conversation_cache(
    question: str,
    threshold: float = 0.7,
    limit: int = 5,
    answer_generator: AnswerGenerator = Depends(get_answer_generator)
):
    """ëŒ€í™” ì´ë ¥ì—ì„œ ìœ ì‚¬í•œ ì§ˆë¬¸ ê²€ìƒ‰"""
    try:
        if not answer_generator.conversation_logger:
            raise HTTPException(status_code=400, detail="ëŒ€í™” ì´ë ¥ ìºì‹œê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        
        similar_questions = answer_generator.conversation_logger.find_similar_questions(
            question, threshold=threshold, limit=limit
        )
        
        results = []
        for log_entry, similarity in similar_questions:
            results.append({
                "id": log_entry.id,
                "question": log_entry.question,
                "answer": log_entry.answer,
                "similarity": similarity,
                "confidence_score": log_entry.confidence_score,
                "question_type": log_entry.question_type,
                "timestamp": log_entry.timestamp,
                "pdf_id": log_entry.pdf_id
            })
        
        return {
            "search_query": question,
            "threshold": threshold,
            "results": results,
            "total_found": len(results)
        }
        
    except Exception as e:
        logger.error(f"ëŒ€í™” ì´ë ¥ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")

# ê°„ë‹¨í•œ ì±—ë´‡ ì—”ë“œí¬ì¸íŠ¸ (PDF ì—†ì´ ì‘ë™)
@app.post("/chat")
async def simple_chat(message: str):
    """ê°„ë‹¨í•œ ì±—ë´‡ ì‘ë‹µ (PDF ì—†ì´ ì‘ë™)"""
    try:
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì‘ë‹µ
        lower_message = message.lower()
        
        if "êµí†µ" in lower_message or "traffic" in lower_message:
            response = "êµí†µ ë°ì´í„°ëŠ” ëŒ€ì‹œë³´ë“œì˜ 'ë¶„ì„' íƒ­ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹ì • êµì°¨ë¡œë¥¼ í´ë¦­í•˜ì‹œë©´ í•´ë‹¹ ì§€ì ì˜ ìƒì„¸í•œ êµí†µëŸ‰ ì •ë³´ë¥¼ ë³¼ ìˆ˜ ìˆì–´ìš”."
        elif "ì‚¬ê³ " in lower_message or "incident" in lower_message:
            response = "êµí†µì‚¬ê³  ì •ë³´ëŠ” 'ì‚¬ê³ ' íƒ­ì—ì„œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë¹¨ê°„ìƒ‰ ì‚¼ê°í˜• ì•„ì´ì½˜ì„ í´ë¦­í•˜ì‹œë©´ ì‚¬ê³  ëª©ë¡ê³¼ ìƒì„¸ ì •ë³´ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        elif "ê²½ë¡œ" in lower_message or "route" in lower_message:
            response = "ê²½ë¡œ ë¶„ì„ì€ 'êµí†µíë¦„' íƒ­ì—ì„œ ì´ìš©í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§€ë„ì—ì„œ ë‘ ì§€ì ì„ ì„ íƒí•˜ì‹œë©´ í•´ë‹¹ êµ¬ê°„ì˜ êµí†µ íë¦„ì„ ë¶„ì„í•´ë“œë¦½ë‹ˆë‹¤."
        elif "ì¦ê²¨ì°¾ê¸°" in lower_message or "favorite" in lower_message:
            response = "ê´€ì‹¬ ìˆëŠ” êµì°¨ë¡œë‚˜ ì‚¬ê³ ë¥¼ ì¦ê²¨ì°¾ê¸°ì— ì¶”ê°€í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³„í‘œ ì•„ì´ì½˜ì„ í´ë¦­í•˜ì‹œë©´ 'ì¦ê²¨ì°¾ê¸°' íƒ­ì—ì„œ ì‰½ê²Œ ì°¾ì•„ë³´ì‹¤ ìˆ˜ ìˆì–´ìš”."
        elif "help" in lower_message or "ë„ì›€" in lower_message or "ì‚¬ìš©ë²•" in lower_message:
            response = "IFRO ëŒ€ì‹œë³´ë“œ ì‚¬ìš©ë²•:\n\nğŸš— ë¶„ì„: êµì°¨ë¡œë³„ êµí†µëŸ‰ ë¶„ì„\nğŸ”„ êµí†µíë¦„: ë‘ ì§€ì  ê°„ ê²½ë¡œ ë¶„ì„\nâš ï¸ ì‚¬ê³ : êµí†µì‚¬ê³  í˜„í™©\nâ­ ì¦ê²¨ì°¾ê¸°: ê´€ì‹¬ ì§€ì  ê´€ë¦¬\nğŸ“Š Tableau: ê³ ê¸‰ ë¶„ì„ ëŒ€ì‹œë³´ë“œ\n\në” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ êµ¬ì²´ì ìœ¼ë¡œ ë¬¼ì–´ë³´ì„¸ìš”!"
        else:
            response = "ë„¤, ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? êµí†µ ë°ì´í„° ë¶„ì„, ëŒ€ì‹œë³´ë“œ ì‚¬ìš©ë²•, íŠ¹ì • ê¸°ëŠ¥ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”!"
        
        return {
            "success": True,
            "response": response,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"ì±—ë´‡ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            "success": False,
            "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.",
            "timestamp": datetime.now().isoformat()
        }

# Django ì—°ë™ì„ ìœ„í•œ ì¶”ê°€ ì—”ë“œí¬ì¸íŠ¸ë“¤

@app.post("/django/ask")
async def django_ask_question(
    question: str,
    pdf_id: str,
    conversation_history: Optional[List[ConversationHistoryItem]] = None,
    vector_store: VectorStoreInterface = Depends(get_vector_store),
    question_analyzer: QuestionAnalyzer = Depends(get_question_analyzer),
    answer_generator: AnswerGenerator = Depends(get_answer_generator)
):
    """Djangoì—ì„œ í˜¸ì¶œí•˜ê¸° ì‰¬ìš´ ì§ˆë¬¸ ì—”ë“œí¬ì¸íŠ¸"""
    
    # ëŒ€í™” ê¸°ë¡ ë³µì› (í•„ìš”í•œ ê²½ìš°)
    if conversation_history:
        question_analyzer.conversation_history.clear()
        for item in conversation_history:
            question_analyzer.add_conversation_item(
                item.question,
                item.answer,
                [],  # ì²­í¬ ì •ë³´ëŠ” ì—†ìŒ
                item.confidence_score
            )
    
    # ê¸°ë³¸ ask ì—”ë“œí¬ì¸íŠ¸ ë¡œì§ ì¬ì‚¬ìš©
    request = QuestionRequest(
        question=question,
        pdf_id=pdf_id,
        use_conversation_context=bool(conversation_history)
    )
    
    return await ask_question(request, vector_store, question_analyzer, answer_generator)

# ì—ëŸ¬ í•¸ë“¤ëŸ¬
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    logger.error(f"ê¸€ë¡œë²Œ ì˜ˆì™¸ ë°œìƒ: {exc}")
    return JSONResponse(
        status_code=500,
        content={"detail": "ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}
    )

# ì„œë²„ ì‹¤í–‰ í•¨ìˆ˜
def run_server(host: str = "0.0.0.0", port: int = 8008, debug: bool = False):
    """
    FastAPI ì„œë²„ ì‹¤í–‰
    
    Args:
        host: ì„œë²„ í˜¸ìŠ¤íŠ¸
        port: ì„œë²„ í¬íŠ¸
        debug: ë””ë²„ê·¸ ëª¨ë“œ
    """
    uvicorn.run(
        "api.endpoints:app",
        host=host,
        port=port,
        reload=debug,
        log_level="info" if not debug else "debug"
    )

if __name__ == "__main__":
    # ê°œë°œ ì„œë²„ ì‹¤í–‰
    run_server(debug=True)
